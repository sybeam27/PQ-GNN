{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import ZINC, Planetoid, Amazon, Coauthor, CitationFull\n",
    "from torch_geometric.loader import DataLoader\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분위수 정보를 노드 특징에 포함 : 분위수 값 자체가 학습을 안정적으로 유도할 수 있도록 추가\n",
    "# 분위수 정보 확장을 위한 데이터 변환\n",
    "# 분위수 정보를 추가하여 GNN이 입력 특성에서 분위수를 학습 가능하도록 변경\n",
    "# 값의 스케일링을 조정(12배 확대)하여 학습 안정성을 증가(Sigmoid 영역 내 활용)\n",
    "# expand()를 사용하여 차원 유지\n",
    "# tau는 분위수 이로 기본적으로 0~1 사이의 값이다.\n",
    "# 하지만 딥러닝 모델은 일반적으로 입력값이 너무 작거나 큰 경우, 학습이 어려워지는 문제가 발생\n",
    "#  0~1 범위의 값은 매우 작기 때문에 신경망의 활성화 함수(예: ReLU, tanh)에서 거의 변화를 일으키지 않을 수도 있습니다.\n",
    "# 많은 머신러닝 모델(특히 신경망)은 입력 특징이 평균 0, 적절한 분산을 가지면 학습이 잘 됨.\n",
    "# 표준 정규분포(평균 0, 표준편차 1)의 경우 ±3σ 구간(약 99.7%의 데이터)은 대략 -3 ~ 3 범위에 있음.\n",
    "# 12는 일반적인 딥러닝 모델이 잘 학습하는 값의 범위(-6 ~ 6)를 만들기 위한 경험적인 값으로 보입니다.\n",
    "\n",
    "def augment_features(x, tau, scale_factor=12):\n",
    "    \"\"\"\n",
    "    분위수 값을 학습 데이터에 추가하여 GNN이 학습할 수 있도록 변환하는 함수.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): 원래 입력 특징값 (batch_size, feature_dim)\n",
    "        tau (torch.Tensor): 분위수 값 (batch_size, 1)\n",
    "        scale_factor (float): 분위수 값의 변환을 조절하는 스케일링 계수 (기본값: 12)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: 변환된 입력 특징값 (batch_size, feature_dim + 1)\n",
    "    \"\"\"\n",
    "    tau = tau.view(-1, 1)  # tau를 (batch_size, 1) 형태로 변환\n",
    "    tau_transformed = (tau - 0.5) * scale_factor  # 분위수 값 변환 (값의 범위를 조절하여 학습 안정성 증가)\n",
    "    return torch.cat((x, tau_transformed.expand(x.size(0), -1)), dim=1)\n",
    "\n",
    "# GCN 구조로 설계\n",
    "# 여러 GNN 구조 중 GCN이 실험적으로 예측 안정성이 더 높은 경우가 많음\n",
    "# GNN 기반 분위수 예측 모델\n",
    "# 이 모델은 그래프 신경망(GNN) 중 하나인 GCN (Graph Convolutional Network) 기반으로, 특정 분위수(Quantile) 값을 예측하는 모델입니다. \n",
    "# 즉, 각 노드의 값을 \"여러 분위수\"로 예측할 수 있도록 설계된 그래프 신경망입니다.\n",
    "class GCNQuantile(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(GCNQuantile, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels + 1, hidden_channels)  # 분위수 값을 포함한 입력\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fc = nn.Linear(hidden_channels, 1) # 단일 분위수 학습을 위한 출력층\n",
    "\n",
    "    def forward(self, x, edge_index, tau):\n",
    "        x = augment_features(x, tau)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true, tau):\n",
    "        diff = y_true - y_pred  # 예측과 실제 값의 차이\n",
    "        loss = torch.where(diff > 0, tau * diff, (1 - tau) * -diff)\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gqnn(model, optimizer, loss_fn, x, edge_index, y,\n",
    "               train_mode = \"single\", tau = 0.1, num_epochs = 10000):\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if train_mode == \"all\":\n",
    "            taus = torch.rand(x.size(0), 1)  # 모든 분위수를 무작위로 샘플링\n",
    "        else:\n",
    "            taus = torch.full(x.size(0, 1), tau)\n",
    "            \n",
    "        y_pred = model(x, edge_index, taus)\n",
    "        loss = loss_fn(y_pred, y, taus)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage 조정 메커니즘 추가 : Coverage 보장이 부족할 경우 신뢰 구간을 자동 조정\n",
    "# Coverage 보장을 위한 동적 분위수 조정 (DQA)\n",
    "# Coverage가 부족하면 신뢰 구간을 넓히고, Coverage가 과하면 구간을 줄이는 방식으로 업데이트\n",
    "def evaluate_coverage(y_true, y_low, y_high):\n",
    "    \"\"\"\n",
    "    현재 모델의 Coverage를 계산하는 함수.\n",
    "    \"\"\"\n",
    "    coverage_exp = ((y_true >= y_low) & (y_true <= y_high)).float().mean().item()\n",
    "    return coverage_exp\n",
    "\n",
    "def dynamic_quantile_adjustment(q_low, q_high, coverage_exp, coverage_target, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    실제 Coverage가 목표 Coverage에 맞도록 분위수를 동적으로 조정하는 함수.\n",
    "    \"\"\"\n",
    "    coverage_error = coverage_target - coverage_exp  # 목표 Coverage와 실제 Coverage 차이 계산\n",
    "    q_low = q_low - learning_rate * coverage_error   # Coverage 부족 시 하한값 조정\n",
    "    q_high = q_high + learning_rate * coverage_error # Coverage 부족 시 상한값 증가\n",
    "    return q_low, q_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분위수 손실 함수 (Coverage + PIW 최적화 포함)\n",
    "# PIW 최적화 추가 : 신뢰 구간 폭을 줄이는 손실항을 추가하여 최적화\n",
    "def coverage_loss(y_true, y_low, y_high, target_coverage=0.9):\n",
    "    \"\"\"\n",
    "    목표 Coverage를 유지하기 위한 손실 함수.\n",
    "    \"\"\"\n",
    "    coverage_exp = ((y_true >= y_low) & (y_true <= y_high)).float().mean()\n",
    "    return (coverage_exp - target_coverage) ** 2\n",
    "\n",
    "def piw_loss(y_low, y_high):\n",
    "    \"\"\"\n",
    "    신뢰 구간 폭(PIW)을 최소화하는 손실 함수.\n",
    "    \"\"\"\n",
    "    return torch.mean(y_high - y_low)  # 구간 폭의 평균을 최소화\n",
    "\n",
    "def total_loss(y_true, y_preds, edge_index, quantiles, target_coverage=0.9, edge_weight=0.1, piw_weight=0.1, coverage_weight=1.0):\n",
    "    \"\"\"\n",
    "    최종 손실 함수:\n",
    "    - Quantile Loss (기본 분위수 손실)\n",
    "    - Graph Regularization (이웃 노드 간 Smoothness 유지)\n",
    "    - Coverage Loss (Coverage 보장)\n",
    "    - PIW Loss (신뢰 구간 폭 최적화)\n",
    "    \"\"\"\n",
    "    y_low, y_mid, y_high = y_preds  # 예측된 분위수 값 분리\n",
    "\n",
    "    # 분위수 손실 (Quantile Loss)\n",
    "    loss_q = sum(QuantileLoss()(y_pred, y_true, q) for y_pred, q in zip(y_preds, quantiles))\n",
    "\n",
    "    # 그래프 정규화 (Graph Smoothness Loss)\n",
    "    src, dst = edge_index\n",
    "    loss_graph = edge_weight * torch.mean((y_preds[1][src] - y_preds[1][dst]) ** 2)\n",
    "\n",
    "    # Coverage 보장 손실\n",
    "    loss_coverage = coverage_weight * coverage_loss(y_true, y_low, y_high, target_coverage)\n",
    "\n",
    "    # 신뢰 구간 폭(PIW) 최적화 손실\n",
    "    loss_piw = piw_weight * piw_loss(y_low, y_high)\n",
    "\n",
    "    return loss_q + loss_graph + loss_coverage + loss_piw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "# Joint Quantile Learning 적용 (Multi-Quantile trainig)\n",
    "# 여러 분위수를 동시에 학습하여 분위수 교차 문제 완화\n",
    "\n",
    "def train_gqnn(data, device, target_coverage=0.9, q=\"all\"):    \n",
    "    model = GCNQuantile(in_channels=data.x.size(1), hidden_channels=16).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "    data = data.to(device)\n",
    "\n",
    "    # 초기 분위수 설정 (q_low: 10%, q_high: 90%)\n",
    "    q_low, q_high = 0.1, 0.9\n",
    "    learning_rate = 0.01  # 분위수 조정 속도\n",
    "\n",
    "    epochs = 1000\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 여러 분위수를 동시에 학습 (Joint Quantile Training)\n",
    "        if q == \"all\":\n",
    "            taus = torch.rand(data.x.size(0), 1).to(device)\n",
    "        else:\n",
    "            taus = torch.full((data.x.size(0), 1), q, device=device)\n",
    "\n",
    "        y_preds = model(data.x, data.edge_index, taus)\n",
    "        y_low = y_preds[:, 0]  # 첫 번째 열이 10% 분위수\n",
    "        y_mid = y_preds[:, 1]  # 두 번째 열이 50% 분위수\n",
    "        y_high = y_preds[:, 2]  # 세 번째 열이 90% 분위수\n",
    "\n",
    "        # 손실 계산 (Coverage + PIW 최적화 포함)\n",
    "        loss = total_loss(data.y, y_preds, data.edge_index, model.quantiles)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 현재 Coverage 측정\n",
    "        coverage_exp = evaluate_coverage(data.y, y_low, y_high)\n",
    "\n",
    "        # Coverage 보장: 분위수 업데이트\n",
    "        q_low, q_high = dynamic_quantile_adjustment(q_low, q_high, coverage_exp, target_coverage, learning_rate)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, Coverage = {coverage_exp:.4f}, PIW = {(y_high - y_low).mean().item():.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 데이터셋 선택 (Cora, Citeseer, Pubmed)\n",
    "dataset_name = \"Cora\"  # \"Citeseer\", \"Pubmed\", \"Computers\", \"Photo\" 등도 가능\n",
    "dataset = Planetoid(root='/tmp/' + dataset_name, name=dataset_name, transform=T.NormalizeFeatures())\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "data = dataset[0]\n",
    "data.y = data.y.float().view(-1, 1)  # 타겟 값을 float형으로 변환 (회귀 수행 가능하도록 변경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Number of features per node: 1433\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of features per node: {data.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")  # 원래 분류 문제이므로 클래스 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 모델 학습 실행 (Coverage 보장 & 신뢰 구간 최적화 포함)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gqnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 31\u001b[0m, in \u001b[0;36mtrain_gqnn\u001b[0;34m(data, device, target_coverage, q)\u001b[0m\n\u001b[1;32m     28\u001b[0m y_high \u001b[38;5;241m=\u001b[39m y_preds[:, \u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# 세 번째 열이 90% 분위수\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 손실 계산 (Coverage + PIW 최적화 포함)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[33], line 33\u001b[0m, in \u001b[0;36mtotal_loss\u001b[0;34m(y_true, y_preds, edge_index, quantiles, target_coverage, edge_weight, piw_weight, coverage_weight)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtotal_loss\u001b[39m(y_true, y_preds, edge_index, quantiles, target_coverage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, edge_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, piw_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, coverage_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    최종 손실 함수:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    - Quantile Loss (기본 분위수 손실)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    - PIW Loss (신뢰 구간 폭 최적화)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     y_low, y_mid, y_high \u001b[38;5;241m=\u001b[39m y_preds  \u001b[38;5;66;03m# 예측된 분위수 값 분리\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# 분위수 손실 (Quantile Loss)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     loss_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(QuantileLoss()(y_pred, y_true, q) \u001b[38;5;28;01mfor\u001b[39;00m y_pred, q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(y_preds, quantiles))\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# GPU 사용 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = data.to(device)\n",
    "\n",
    "# 모델 학습 실행 (Coverage 보장 & 신뢰 구간 최적화 포함)\n",
    "model = train_gqnn(data, device, q=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gqnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
