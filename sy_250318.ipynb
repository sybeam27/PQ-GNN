{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GCNConv, GraphSAGE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_data(num_nodes=1000):\n",
    "    np.random.seed(1127)\n",
    "    torch.manual_seed(1127)\n",
    "    \n",
    "    # 비선형 데이터\n",
    "    X = np.random.rand(num_nodes, 5)  # 5차원\n",
    "    y = np.sin(X[:, 0] * 3) + 0.1 * np.random.rand(num_nodes)\n",
    "    \n",
    "    edge_index = torch.randint(0, num_nodes, (2, num_nodes * 2))\n",
    "    \n",
    "    # PyG 데이터 변환\n",
    "    return Data(\n",
    "        x = torch.tensor(X, dtype = torch.float32),\n",
    "        edge_index= edge_index,\n",
    "        y = torch.tensor(y, dtype = torch.float32).unsqueeze(1),\n",
    "    )\n",
    "\n",
    "def generate_noisy_graph_data(num_nodes=1000, noise_type=\"gaussian\", noise_level=0.1, outlier_ratio=0.05):\n",
    "    \"\"\"\n",
    "    다양한 노이즈를 추가하여 그래프 데이터를 생성하는 함수\n",
    "\n",
    "    Args:\n",
    "    - num_nodes (int): 노드 개수\n",
    "    - noise_type (str): 추가할 노이즈 유형 (\"gaussian\", \"uniform\", \"outlier\", \"edge_noise\")\n",
    "    - noise_level (float): 노이즈의 강도 (가우시안 및 유니폼 노이즈)\n",
    "    - outlier_ratio (float): 이상치(outlier) 비율\n",
    "\n",
    "    Returns:\n",
    "    - PyG Data 객체\n",
    "    \"\"\"\n",
    "    np.random.seed(1127)\n",
    "    torch.manual_seed(1127)\n",
    "    \n",
    "    X = np.random.rand(num_nodes, 5)  # 5차원 특징\n",
    "    y = np.sin(X[:, 0] * 3) + 0.1 * np.random.rand(num_nodes)  # 기본 타겟\n",
    "    \n",
    "    if noise_type == \"gaussian\":\n",
    "        y += np.random.normal(0, noise_level, size=num_nodes)\n",
    "    elif noise_type == \"uniform\":\n",
    "        y += np.random.uniform(-noise_level, noise_level, size=num_nodes)\n",
    "    elif noise_type == \"outlier\":\n",
    "        num_outliers = int(num_nodes * outlier_ratio)\n",
    "        outlier_indices = np.random.choice(num_nodes, num_outliers, replace=False)\n",
    "        y[outlier_indices] += np.random.normal(3, 1.0, size=num_outliers)  # 극단적인 변화\n",
    "\n",
    "    # 그래프 구조적 노이즈 (엣지 변경)\n",
    "    edge_index = torch.randint(0, num_nodes, (2, num_nodes * 2))\n",
    "    if noise_type == \"edge_noise\":\n",
    "        # 엣지에 무작위 잡음을 추가하여 구조적 변형 수행\n",
    "        num_noisy_edges = int(edge_index.shape[1] * noise_level)\n",
    "        noise_indices = np.random.choice(edge_index.shape[1], num_noisy_edges, replace=False)\n",
    "        edge_index[:, noise_indices] = torch.randint(0, num_nodes, (2, num_noisy_edges))\n",
    "\n",
    "    return Data(\n",
    "        x=torch.tensor(X, dtype=torch.float32),\n",
    "        edge_index=edge_index,\n",
    "        y=torch.tensor(y, dtype=torch.float32).unsqueeze(1),\n",
    "    )\n",
    "\n",
    "def max_normalize(x):\n",
    "    return x / np.max(np.abs(x)) if np.max(np.abs(x)) != 0 else x\n",
    "\n",
    "def std_normalize(x):\n",
    "    return (x - np.mean(x)) / np.std(x) if np.std(x) != 0 else np.zeros(len(x))\n",
    "\n",
    "def int_normalize(x):\n",
    "    return ((x - np.min(x)) / (np.max(x) - np.min(x)) * 2 - 1) if np.std(x) != 0 else np.zeros(len(x))\n",
    "\n",
    "def simulate_ising(n, h0, J):\n",
    "    G = nx.grid_2d_graph(n, n)\n",
    "    l = np.linspace(-1.0, 1.0, n)\n",
    "    \n",
    "    s = np.random.choice([-1, 1], size=(n, n))\n",
    "    # Placeholder for metropolis algorithm\n",
    "    y = s.flatten()\n",
    "    f = [[l[i], l[j]] for j in range(n) for i in range(n)]\n",
    "    \n",
    "    return G, [nx.to_scipy_sparse_matrix(G)], y, f\n",
    "\n",
    "def parse_mean_fill(series, normalize=False):\n",
    "    series = series.replace({',': ''}, regex=True)\n",
    "    series = pd.to_numeric(series, errors='coerce')\n",
    "    mean_val = series.mean()\n",
    "    series.fillna(mean_val, inplace=True)\n",
    "    \n",
    "    if normalize:\n",
    "        series = (series - mean_val) / series.std()\n",
    "    \n",
    "    return series.values\n",
    "\n",
    "def read_county(prediction, year):\n",
    "    adj = pd.read_csv(\"dataset/election/adjacency.txt\", header=None, sep=\"\\t\", dtype=str, encoding=\"ISO-8859-1\")\n",
    "    fips2cty = {row[1]: row[0] for _, row in adj.iterrows() if pd.notna(row[1])}\n",
    "    \n",
    "    hh = adj.iloc[:, 1].ffill().astype(int)\n",
    "    tt = adj.iloc[:, 3].astype(int)\n",
    "    \n",
    "    fips = sorted(set(hh).union(set(tt)))\n",
    "    id2num = {id_: num for num, id_ in enumerate(fips)}\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(len(id2num)))\n",
    "    G.add_edges_from([(id2num[h], id2num[t]) for h, t in zip(hh, tt)])\n",
    "    \n",
    "    # Load datasets\n",
    "    VOT = pd.read_csv(\"dataset/election/election.csv\")\n",
    "    ICM = pd.read_csv(\"dataset/election/income.csv\")\n",
    "    POP = pd.read_csv(\"dataset/election/population.csv\")\n",
    "    EDU = pd.read_csv(\"dataset/election/education.csv\")\n",
    "    UEP = pd.read_csv(\"dataset/election/unemployment.csv\")\n",
    "    \n",
    "    cty = pd.DataFrame({'FIPS': fips, 'County': [fips2cty.get(f, '') for f in fips]})\n",
    "    vot = VOT[['fips_code', f'dem_{year}', f'gop_{year}']].rename(columns={'fips_code': 'FIPS'})\n",
    "    icm = ICM[['FIPS', f'MedianIncome{min(max(2011, year), 2018)}']]\n",
    "    pop = POP[['FIPS', f'R_NET_MIG_{min(max(2011, year), 2018)}', f'R_birth_{min(max(2011, year), 2018)}', f'R_death_{min(max(2011, year), 2018)}']]\n",
    "    edu = EDU[['FIPS', f'BachelorRate{year}']]\n",
    "    uep = UEP[['FIPS', f'Unemployment_rate_{min(max(2007, year), 2018)}']]\n",
    "    \n",
    "    dat = cty.merge(vot, on='FIPS', how='left')\n",
    "    dat = dat.merge(icm, on='FIPS', how='left')\n",
    "    dat = dat.merge(pop, on='FIPS', how='left')\n",
    "    dat = dat.merge(edu, on='FIPS', how='left')\n",
    "    dat = dat.merge(uep, on='FIPS', how='left')\n",
    "    \n",
    "    # Extract features and labels\n",
    "    dem = parse_mean_fill(dat.iloc[:, 2])\n",
    "    gop = parse_mean_fill(dat.iloc[:, 3])\n",
    "    \n",
    "    ff = np.zeros((len(dat), 7), dtype=np.float32)\n",
    "    for i in range(6):\n",
    "        ff[:, i] = parse_mean_fill(dat.iloc[:, i + 4], normalize=True)\n",
    "    \n",
    "    ff[:, 6] = (gop - dem) / (gop + dem)\n",
    "    \n",
    "    label_mapping = {\n",
    "        \"income\": 0, \"migration\": 1, \"birth\": 2, \"death\": 3,\n",
    "        \"education\": 4, \"unemployment\": 5, \"election\": 6\n",
    "    }\n",
    "    \n",
    "    if prediction not in label_mapping:\n",
    "        raise ValueError(\"Unexpected prediction type\")\n",
    "    \n",
    "    pos = label_mapping[prediction]\n",
    "    y = ff[:, pos]\n",
    "    f = [np.concatenate((ff[i, :pos], ff[i, pos + 1:])) for i in range(len(dat))]\n",
    "    \n",
    "    return G, [csr_matrix(nx.adjacency_matrix(G))], y, f\n",
    "\n",
    "def load_county_graph_data(prediction: str, year: int):\n",
    "    G, A, labels, feats = read_county(prediction, year)\n",
    "\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    pyg_data = from_networkx(G)\n",
    "\n",
    "    edge_index = pyg_data.edge_index\n",
    "    sorted_edges = torch.sort(edge_index, dim=0)[0]  # (u, v)와 (v, u)를 정렬\n",
    "    unique_edges = torch.unique(sorted_edges, dim=1)  # 고유 엣지만 유지\n",
    "    pyg_data.edge_index = unique_edges  # 중복 제거된 edge_index 적용\n",
    "\n",
    "    pyg_data.x = torch.tensor(feats, dtype=torch.float)\n",
    "\n",
    "    pyg_data.y = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    return pyg_data\n",
    "\n",
    "def read_transportation_network(network_name, net_skips, net_cols, netf_cols, flow_skips, flow_cols, V_range):\n",
    "    # Load data\n",
    "    dat_net = pd.read_csv(f\"dataset/transportation/{network_name}/{network_name}_net.tntp\", \n",
    "                           skiprows=net_skips, sep='\\s+', usecols=net_cols, header=None).values\n",
    "    dat_netf = pd.read_csv(f\"dataset/transportation/{network_name}/{network_name}_net.tntp\", \n",
    "                            skiprows=net_skips, sep='\\s+', usecols=netf_cols, header=None).values\n",
    "    dat_flow = pd.read_csv(f\"dataset/transportation/{network_name}/{network_name}_flow.tntp\", \n",
    "                            skiprows=flow_skips, sep='\\s+', usecols=flow_cols, header=None).values\n",
    "    \n",
    "    # Map node labels to indices\n",
    "    lb2id = {v: i for i, v in enumerate(V_range, start=1)}\n",
    "    NV = len(V_range)\n",
    "    \n",
    "    # Create directed graph\n",
    "    g = nx.DiGraph()\n",
    "    g.add_nodes_from(range(1, NV + 1))\n",
    "    \n",
    "    for src, dst in dat_net:\n",
    "        if src in lb2id and dst in lb2id:\n",
    "            g.add_edge(lb2id[src], lb2id[dst])\n",
    "    \n",
    "    # Edge labels\n",
    "    flow_dict = {}\n",
    "    for src, dst, flow in dat_flow:\n",
    "        if src in lb2id and dst in lb2id:\n",
    "            flow_dict[(lb2id[src], lb2id[dst])] = flow\n",
    "    \n",
    "    y = np.array([flow_dict.get((e[0], e[1]), 0) for e in g.edges()])\n",
    "    y = (y - np.mean(y)) / np.std(y)  # Standard normalization\n",
    "    \n",
    "    # Edge features\n",
    "    netf_dict = {}\n",
    "    for i in range(len(dat_net)):\n",
    "        src, dst = dat_net[i]\n",
    "        if src in lb2id and dst in lb2id:\n",
    "            netf_dict[(lb2id[src], lb2id[dst])] = dat_netf[i]\n",
    "    \n",
    "    ff = np.array([netf_dict[e] for e in g.edges()])\n",
    "    mean_ff = np.mean(ff, axis=0)\n",
    "    std_ff = np.std(ff, axis=0)\n",
    "    std_ff[std_ff == 0] = 1  # Prevent division by zero\n",
    "    netf = (ff - mean_ff) / std_ff  \n",
    "    \n",
    "    f = list(netf)\n",
    "    \n",
    "    # Line graph transformation\n",
    "    G1 = nx.Graph()\n",
    "    G2 = nx.Graph()\n",
    "    sorted_edges = sorted(g.edges())\n",
    "    tuple2id = {e: i for i, e in enumerate(sorted_edges)}\n",
    "    \n",
    "    for u in g.nodes:\n",
    "        innbrs = list(g.predecessors(u))\n",
    "        outnbrs = list(g.successors(u))\n",
    "        \n",
    "        for v in innbrs:\n",
    "            for w in outnbrs:\n",
    "                if (v, u) in tuple2id and (u, w) in tuple2id:\n",
    "                    G1.add_edge(tuple2id[(v, u)], tuple2id[(u, w)])\n",
    "        \n",
    "        for v in innbrs:\n",
    "            for w in innbrs:\n",
    "                if w > v and (v, u) in tuple2id and (w, u) in tuple2id:\n",
    "                    G2.add_edge(tuple2id[(v, u)], tuple2id[(w, u)])\n",
    "        \n",
    "        for v in outnbrs:\n",
    "            for w in outnbrs:\n",
    "                if w > v and (u, v) in tuple2id and (u, w) in tuple2id:\n",
    "                    G2.add_edge(tuple2id[(u, v)], tuple2id[(u, w)])\n",
    "                    \n",
    "    size = max(len(G1.nodes), len(G2.nodes))\n",
    "    A1 = np.zeros((size, size))\n",
    "    A2 = np.zeros((size, size))\n",
    "    \n",
    "    A1[:nx.number_of_nodes(G1), :nx.number_of_nodes(G1)] = nx.adjacency_matrix(G1).todense()\n",
    "    A2[:nx.number_of_nodes(G2), :nx.number_of_nodes(G2)] = nx.adjacency_matrix(G2).todense()\n",
    "    \n",
    "    A = A1 + A2\n",
    "    \n",
    "    return nx.Graph(A), A, y, f\n",
    "\n",
    "def load_trans_graph_data(city: str):\n",
    "    if city == 'Anaheim':\n",
    "        G, A, labels, feats = read_transportation_network(city, 8, [0, 1], [2, 3, 4, 7], 6, [0, 1, 3], range(1, 417))\n",
    "    elif city == 'ChicagoSketch':\n",
    "        G, A, labels, feats = read_transportation_network(city, 7, [0, 1], [2, 3, 4, 7], 1, [0, 1, 2], range(388, 934))\n",
    "\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    pyg_data = from_networkx(G)\n",
    "\n",
    "    edge_index = pyg_data.edge_index\n",
    "    sorted_edges = torch.sort(edge_index, dim=0)[0]  # (u, v)와 (v, u)를 정렬\n",
    "    unique_edges = torch.unique(sorted_edges, dim=1)  # 고유 엣지만 유지\n",
    "    pyg_data.edge_index = unique_edges  # 중복 제거된 edge_index 적용\n",
    "\n",
    "    pyg_data.x = torch.tensor(feats, dtype=torch.float)\n",
    "\n",
    "    pyg_data.y = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    return pyg_data\n",
    "\n",
    "def read_twitch_network(cnm, dim_reduction=False, dim_embed=8):\n",
    "    feats_all = []\n",
    "    countries = [\"DE\", \"ENGB\", \"ES\", \"FR\", \"PTBR\", \"RU\"]\n",
    "    \n",
    "    for cn in countries:\n",
    "        with open(f\"dataset/twitch/{cn}/musae_{cn}_features.json\", \"r\") as f:\n",
    "            feats = json.load(f)\n",
    "        feats_all.extend(feats.values())\n",
    "\n",
    "    ndim = max(np.concatenate(feats_all)) + 1\n",
    "\n",
    "    def feat_encode(feat_list):\n",
    "        \"\"\"특징 벡터를 원핫 인코딩 형태로 변환\"\"\"\n",
    "        vv = np.zeros(ndim, dtype=np.float32)\n",
    "        valid_indices = np.array(feat_list)\n",
    "        \n",
    "        if np.any(valid_indices >= ndim):\n",
    "            raise ValueError(f\"Index out of bounds! Max index: {max(valid_indices)}, ndim: {ndim}\")\n",
    "        \n",
    "        vv[valid_indices] = 1.0\n",
    "        return vv\n",
    "\n",
    "    f_all = list(map(feat_encode, feats_all))\n",
    "\n",
    "    with open(f\"dataset/twitch/{cnm}/musae_{cnm}_features.json\", \"r\") as f:\n",
    "        feats = json.load(f)\n",
    "\n",
    "    id2ft = {int(k) + 1: v for k, v in feats.items()}\n",
    "    n = len(id2ft)\n",
    "    assert min(id2ft.keys()) == 1 and max(id2ft.keys()) == n\n",
    "\n",
    "    f = [feat_encode(id2ft[i]) for i in sorted(id2ft.keys())]\n",
    "\n",
    "    if dim_reduction:\n",
    "        f_matrix = np.stack(f_all, axis=1)\n",
    "        U, S, Vt = svds(f_matrix, k=dim_embed)\n",
    "        U *= np.sign(np.sum(U, axis=0))  # sign correction\n",
    "        f = [U.T @ f_ for f_ in f]\n",
    "\n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(range(1, len(f) + 1))\n",
    "\n",
    "    links = pd.read_csv(f\"dataset/twitch/{cnm}/musae_{cnm}_edges.csv\")\n",
    "    for _, row in links.iterrows():\n",
    "        g.add_edge(row[\"from\"] + 1, row[\"to\"] + 1)\n",
    "\n",
    "    trgts = pd.read_csv(f\"dataset/twitch/{cnm}/musae_{cnm}_target.csv\")\n",
    "    nid2views = dict(zip(trgts[\"new_id\"], trgts[\"views\"]))\n",
    "    y = std_normalize(np.log([nid2views[i - 1] + 1.0 for i in range(1, g.number_of_nodes() + 1)]))\n",
    "\n",
    "    return g, [csr_matrix(nx.adjacency_matrix(g))], y, f\n",
    "\n",
    "def load_twitch_graph_data(cnm: str):\n",
    "    G, A, labels, feats = read_twitch_network(cnm)\n",
    "\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    pyg_data = from_networkx(G)\n",
    "\n",
    "    edge_index = pyg_data.edge_index\n",
    "    sorted_edges = torch.sort(edge_index, dim=0)[0]  # (u, v)와 (v, u)를 정렬\n",
    "    unique_edges = torch.unique(sorted_edges, dim=1)  # 고유 엣지만 유지\n",
    "    pyg_data.edge_index = unique_edges  # 중복 제거된 edge_index 적용\n",
    "\n",
    "    pyg_data.x = torch.tensor(feats, dtype=torch.float)\n",
    "\n",
    "    pyg_data.y = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    return pyg_data\n",
    "\n",
    "def load_wiki_graph_data(category):\n",
    "    edge_path = f'dataset/wikipedia/{category}/musae_{category}_edges.csv'\n",
    "    feature_path = f'dataset/wikipedia/{category}/musae_{category}_features.json'\n",
    "    target_path = f'dataset/wikipedia/{category}/musae_{category}_target.csv'\n",
    "    \n",
    "    # 엣지 데이터 로드\n",
    "    edge_df = pd.read_csv(edge_path)\n",
    "    edge_index = torch.tensor(edge_df.values.T, dtype=torch.long)\n",
    "    \n",
    "    # 피처 데이터 로드\n",
    "    with open(feature_path, \"r\") as f:\n",
    "        features_dict = json.load(f)\n",
    "    \n",
    "    node_ids = sorted(map(int, features_dict.keys()))  # 노드 ID 정렬\n",
    "    node_id_map = {old_id: new_id for new_id, old_id in enumerate(node_ids)}\n",
    "    \n",
    "    num_nodes = len(node_ids)\n",
    "    num_features = max(max(v) for v in features_dict.values()) + 1  # 가장 큰 feature index 찾기\n",
    "    x = torch.zeros((num_nodes, num_features), dtype=torch.float32)\n",
    "    \n",
    "    for node, features in features_dict.items():\n",
    "        new_id = node_id_map[int(node)]  # 노드 ID 변환\n",
    "        x[new_id, features] = 1.0  # One-hot 인코딩\n",
    "    \n",
    "    # 타겟 데이터 로드\n",
    "    target_df = pd.read_csv(target_path)\n",
    "    target_df[\"id\"] = target_df[\"id\"].map(node_id_map)  # 노드 ID 변환\n",
    "    target_df = target_df.dropna().astype(int)  # 변환되지 않은 노드 제거\n",
    "    \n",
    "    # y = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    y = torch.zeros((num_nodes, 1), dtype=torch.long)  # [, 1] 형태로 변경\n",
    "    y[target_df[\"id\"].values] = torch.tensor(target_df[\"target\"].values, dtype=torch.long).view(-1, 1)\n",
    "    \n",
    "    # PyG Data 객체 생성\n",
    "    graph_data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    return graph_data\n",
    "\n",
    "def set_seed(seed=1127):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    \"\"\"PyTorch Tensor → NumPy 변환 후 1차원으로 변형\"\"\"\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        return tensor.cpu().numpy().squeeze()\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.squeeze()\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(tensor)}\")\n",
    " \n",
    "def sort_by_y(x, y, *intervals):\n",
    "    sort_idx = np.argsort(to_numpy(y).ravel())  # Y값을 기준으로 정렬할 인덱스\n",
    "    sorted_x = to_numpy(x).ravel()[sort_idx]\n",
    "    sorted_y = to_numpy(y).ravel()[sort_idx]\n",
    "    sorted_intervals = [to_numpy(interval).ravel()[sort_idx] for interval in intervals]\n",
    "    return sorted_x, sorted_y, sorted_intervals\n",
    "\n",
    "def split_graph_data(graph_data, test_ratio=0.2):\n",
    "    transform = RandomNodeSplit(split=\"train_rest\", num_val=0.0, num_test=test_ratio)\n",
    "    data = transform(graph_data)\n",
    "\n",
    "    # Train/Test 마스크 생성\n",
    "    train_mask = data.train_mask.to(data.x.device)\n",
    "    test_mask = data.test_mask.to(data.x.device)\n",
    "\n",
    "    # 훈련 및 테스트 데이터 선택\n",
    "    train_data = data.clone()\n",
    "    test_data = data.clone()\n",
    "\n",
    "    train_data.x = data.x[train_mask]\n",
    "    train_data.y = data.y[train_mask]\n",
    "    test_data.x = data.x[test_mask]\n",
    "    test_data.y = data.y[test_mask]\n",
    "\n",
    "    # 노드 인덱스를 매핑하여 edge_index 수정 (훈련 데이터)\n",
    "    train_node_mapping = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(train_mask.nonzero(as_tuple=True)[0])}\n",
    "    train_edges_mask = train_mask[data.edge_index[0]] & train_mask[data.edge_index[1]]\n",
    "    train_edges = data.edge_index[:, train_edges_mask]\n",
    "\n",
    "    train_data.edge_index = torch.stack([\n",
    "        torch.tensor([train_node_mapping[i.item()] for i in train_edges[0]], dtype=torch.long, device=data.x.device),\n",
    "        torch.tensor([train_node_mapping[i.item()] for i in train_edges[1]], dtype=torch.long, device=data.x.device)\n",
    "    ], dim=0)\n",
    "\n",
    "    # 노드 인덱스를 매핑하여 edge_index 수정 (테스트 데이터)\n",
    "    test_node_mapping = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(test_mask.nonzero(as_tuple=True)[0])}\n",
    "    test_edges_mask = test_mask[data.edge_index[0]] & test_mask[data.edge_index[1]]\n",
    "    test_edges = data.edge_index[:, test_edges_mask]\n",
    "\n",
    "    test_data.edge_index = torch.stack([\n",
    "        torch.tensor([test_node_mapping[i.item()] for i in test_edges[0]], dtype=torch.long, device=data.x.device),\n",
    "        torch.tensor([test_node_mapping[i.item()] for i in test_edges[1]], dtype=torch.long, device=data.x.device)\n",
    "    ], dim=0)\n",
    "\n",
    "    print(f\"Train data: {train_data.x.shape[0]} nodes, {train_data.edge_index.shape[1]} edges\")\n",
    "    print(f\"Test data: {test_data.x.shape[0]} nodes, {test_data.edge_index.shape[1]} edges\")\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def augment_features(x, tau):\n",
    "    if isinstance(tau, float):  # tau가 float이면 변환\n",
    "        tau = torch.tensor([tau])\n",
    "    \n",
    "    tau = tau.view(-1, 1)\n",
    "    tau_transformed = (tau - 0.5) * 12 # 분위수 값 변환: 학습 안정성 증가\n",
    "    \n",
    "    return torch.cat((x, tau_transformed.expand(x.size(0), -1)), dim = 1)\n",
    "\n",
    "def coverage_width(y_true, y_low, y_upper):\n",
    "    coverage = ((y_true >= y_low) & (y_true <= y_upper)).float().mean()\n",
    "    width = (y_upper - y_low).float().abs().mean()\n",
    "    \n",
    "    return coverage, width\n",
    "    \n",
    "def evaluate_model_performance(preds_low, preds_upper, targets, target=0.9):\n",
    "    coverage = np.mean((targets >= preds_low) & (targets <= preds_upper))\n",
    "    \n",
    "    interval_width = np.mean(preds_upper - preds_low)\n",
    "    normalized_interval_width = interval_width / (np.max(targets) - np.min(targets))\n",
    "    \n",
    "    median_pred = (preds_low + preds_upper) / 2    # 신뢰구간 중앙값\n",
    "    mpe = np.mean(np.abs(median_pred - targets)) # 예측 구간 중심이 실제값과 얼마나 가까운지\n",
    "    \n",
    "    sharpness = np.mean(np.square(preds_upper - preds_low)) # 예측 구간의 날카로움: 제곱을 사용해 큰 폭일수록 더 강한 패널티\n",
    "    \n",
    "    alpha = 0.5\n",
    "    penalties = np.where(targets < preds_low, preds_low - targets, np.where(targets > preds_upper, targets - preds_upper, 0))  \n",
    "    winkler = np.mean(interval_width + 2 * alpha * penalties) # 신뢰구간이 실제값을 포함하지 않으면 패널티 적용\n",
    "    \n",
    "    MCT = interval_width * abs(coverage - target)     # 조정된 Coverage Tradeoff 지표 (MCT)\n",
    "\n",
    "    print(f\"예측 관련 - Coverage Rate (CR) ⬆: {coverage:.4f} (목표: {target}), Mean Prediction Error (MPE) ⬇: {mpe:.4f}\")\n",
    "    print(f\"구간 관련 - Interval Width (IW) ⬇: {interval_width:.4f}, Sharpness ⬇: {sharpness:.4f}, Winkler Score (WS) ⬇: {winkler:.4f}\")\n",
    "    print(f\"종합 - MisCoverage Trade-off (MCT) ⬇: {MCT:.4f}\")\n",
    "    \n",
    "    # return {\n",
    "    #     \"Coverage Rate (CR)\": coverage,\n",
    "    #     \"Interval Width (IW)\": interval_width,\n",
    "    #     \"Normalized IW\": normalized_interval_width,\n",
    "    #     \"Mean Prediction Error (MPE)\": mpe,\n",
    "    #     \"Sharpness\": sharpness,\n",
    "    #     \"Winkler Score (WS)\": winkler,\n",
    "    #     \"MisCoverage Trade-off (MCT)\": MCT\n",
    "    # }\n",
    "\n",
    "set_seed(1127)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQNN_R(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim+1, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index, tau):\n",
    "        x = augment_features(x, tau)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        return self.fc(x)\n",
    "    \n",
    "class GQNN_N(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        return self.fc(x)\n",
    "    \n",
    "class GQNN_D(nn.Module):\n",
    "    # tau와 preds 같이 출력\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.fc_tau = nn.Linear(hidden_dim, 2)\n",
    "        self.fc_pred = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        taus = torch.sigmoid(self.fc_tau(x))\n",
    "        tau_low = 0.05 + 0.45 * torch.sigmoid(taus[:, 0:1])  # 0.05 ~ 0.5\n",
    "        tau_upper = 0.5 + 0.45 * torch.sigmoid(taus[:, 1:2])  # 0.5 ~ 0.95\n",
    "        \n",
    "        preds = self.fc_pred(x)\n",
    "        # scale = torch.log1p(torch.abs(preds))  # 작을 때는 작은 값, 클 때는 점진적으로 커지는 특징\n",
    "        scale = F.relu(torch.log1p(torch.abs(preds)) + 0.1)  # 최소값 0.1 보장\n",
    "        preds_low = preds - tau_low * scale\n",
    "        preds_upper = preds + tau_upper * scale\n",
    "        \n",
    "        return preds_low, preds_upper, tau_low, tau_upper\n",
    "\n",
    "class GNN_CP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        return self.fc(x)\n",
    "    \n",
    "class QRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, y_pred, y_true, tau):\n",
    "        diff = y_true - y_pred\n",
    "        loss = torch.where(diff > 0, tau * diff, (tau - 1) * diff)\n",
    "        \n",
    "        return torch.mean(loss)\n",
    "    \n",
    "class RQRLoss(nn.Module):\n",
    "    def __init__(self, target=0.9, lambda_factor=1):\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.lf = lambda_factor\n",
    "    \n",
    "    def forward(self, preds, target):\n",
    "        q1, q2 = preds[:, 0], preds[:, 1]\n",
    "        diff1 = target - q1\n",
    "        diff2 = target - q2\n",
    "        width = q2 - q1\n",
    "        \n",
    "        rqr_loss = torch.maximum(diff1 * diff2 * (self.target + 2 * self.lf),\n",
    "                                diff2 * diff1 * (self.target + 2 * self.lf - 1))\n",
    "        \n",
    "        width_loss = self.lf * torch.square(width) * 0.5\n",
    "        \n",
    "        return torch.mean(rqr_loss + width_loss)\n",
    "    \n",
    "class IQRLoss(nn.Module):\n",
    "    def __init__(self, target_coverage=0.9):\n",
    "        super().__init__()\n",
    "        self.alpha = target_coverage\n",
    "        \n",
    "    def forward(self, preds_low, preds_upper, tau_low, tau_upper, target):\n",
    "        # Quantile Loss\n",
    "        diff1 = target - preds_low\n",
    "        loss1 = torch.where(diff1 > 0, tau_low * diff1, (tau_low - 1) * diff1)\n",
    "        # 하한 손실에 1.5배 가중치: 하한 예측 더 신중하게 학습하도록\n",
    "        # loss1 = torch.where(diff1 > 0, 1.5 * tau_low * diff1, 1.5 * (tau_low - 1) * diff1) \n",
    "        diff2 = target - preds_upper\n",
    "        loss2 = torch.where(diff2 > 0, tau_upper * diff2, (tau_upper - 1) * diff2)\n",
    "        qr_loss = loss1 + loss2\n",
    "        \n",
    "        # Coverage Loss\n",
    "        coverage_loss = torch.maximum(diff1 * diff2 * (self.alpha + 2), diff2 * diff1 * (self.alpha + 1)) \n",
    "        width_penalty = torch.square(preds_upper - preds_low) * 0.5\n",
    "        \n",
    "        loss = qr_loss + coverage_loss + width_penalty\n",
    "        return torch.mean(loss)\n",
    "\n",
    "class IQRLossWithCoverage(nn.Module):\n",
    "    def __init__(self, target_coverage=0.9, coverage_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.target_coverage = target_coverage  # 목표 커버리지 (예: 0.9 for 90%)\n",
    "        self.coverage_weight = coverage_weight  # 커버리지 손실 가중치\n",
    "        \n",
    "    def forward(self, preds_low, preds_upper, target, tau_low, tau_upper):\n",
    "        # 기존 Quantile Loss 계산\n",
    "        diff1 = target - preds_low\n",
    "        diff2 = target - preds_upper\n",
    "        loss1 = torch.where(diff1 > 0, tau_low * diff1, (tau_low - 1) * diff1)\n",
    "        loss2 = torch.where(diff2 > 0, tau_upper * diff2, (tau_upper - 1) * diff2)\n",
    "        quantile_loss = torch.mean(loss1 + loss2)\n",
    "        \n",
    "        # 커버리지 계산\n",
    "        in_coverage = (preds_low <= target) & (target <= preds_upper)  # 구간 내 포함 여부\n",
    "        actual_coverage = torch.mean(in_coverage.float())  # 실제 커버리지 비율\n",
    "        \n",
    "        # 커버리지 손실: 목표 커버리지와의 차이를 벌점으로 추가\n",
    "        coverage_loss = torch.abs(self.target_coverage - actual_coverage)\n",
    "        \n",
    "        # 총 손실: Quantile Loss + Coverage Loss\n",
    "        total_loss = quantile_loss + self.coverage_weight * coverage_loss\n",
    "        \n",
    "        return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2193019/2241754578.py:162: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  pyg_data.x = torch.tensor(feats, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# 기본 비선형 그래프 데이터\n",
    "graph_data_basic = generate_graph_data(num_nodes=1000)\n",
    "\n",
    "# 노이즈 비선형 그래프 데이터\n",
    "graph_data_noise_gaussian = generate_noisy_graph_data(num_nodes=1000, noise_type='gaussian', noise_level=0.3)\n",
    "graph_data_noise_uniform = generate_noisy_graph_data(num_nodes=1000, noise_type='uniform', noise_level=0.3)\n",
    "graph_data_noise_outlier = generate_noisy_graph_data(num_nodes=1000, noise_type='outlier', noise_level=0.3)\n",
    "graph_data_noise_edge = generate_noisy_graph_data(num_nodes=1000, noise_type='edge_noise', noise_level=0.3)\n",
    "\n",
    "# County 데이터셋\n",
    "graph_data_county_edu = load_county_graph_data('education', 2012)\n",
    "graph_data_county_elec = load_county_graph_data('election', 2012)\n",
    "graph_data_county_inc = load_county_graph_data('income', 2012)\n",
    "graph_data_county_unemp = load_county_graph_data('unemployment', 2012)\n",
    "\n",
    "# Twitch 데이터셋\n",
    "graph_data_twitch_de = load_twitch_graph_data('DE')\n",
    "graph_data_twitch_engb = load_twitch_graph_data('ENGB')\n",
    "graph_data_twitch_es = load_twitch_graph_data('ES')\n",
    "graph_data_twitch_fr = load_twitch_graph_data('FR')\n",
    "graph_data_twitch_ptbr = load_twitch_graph_data('PTBR')\n",
    "graph_data_twitch_ru = load_twitch_graph_data('RU')\n",
    "\n",
    "# Wikipedia 데이터\n",
    "graph_data_wiki_ch = load_wiki_graph_data('chameleon')\n",
    "graph_data_wiki_cr = load_wiki_graph_data('crocodile')\n",
    "graph_data_wiki_sq = load_wiki_graph_data('squirrel')\n",
    "\n",
    "# Transfortation 데이터셋\n",
    "graph_data_trans_ana = load_trans_graph_data('Anaheim')\n",
    "graph_data_trans_chica = load_trans_graph_data('ChicagoSketch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_graph_data(graph_data, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    GNN용 Train-Test Split (edge_index를 올바르게 재매핑하여 유지)\n",
    "    \"\"\"\n",
    "    num_nodes = graph_data.x.shape[0]  # 전체 노드 개수\n",
    "    num_test = int(num_nodes * test_ratio)  # 테스트 데이터 노드 개수\n",
    "\n",
    "    # 랜덤하게 Train/Test 노드 인덱스 선택\n",
    "    indices = torch.randperm(num_nodes)\n",
    "    test_nodes = indices[:num_test]\n",
    "    train_nodes = indices[num_test:]\n",
    "\n",
    "    # 노드 마스크 생성\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_nodes] = True\n",
    "    test_mask[test_nodes] = True\n",
    "\n",
    "    # Train/Test용 edge_index 필터링\n",
    "    train_edge_mask = train_mask[graph_data.edge_index[0]] & train_mask[graph_data.edge_index[1]]\n",
    "    test_edge_mask = test_mask[graph_data.edge_index[0]] & test_mask[graph_data.edge_index[1]]\n",
    "\n",
    "    # Train용 edge_index와 노드 인덱스 재매핑\n",
    "    train_edge_index = graph_data.edge_index[:, train_edge_mask]\n",
    "    train_node_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(train_nodes)}\n",
    "    train_edge_index = torch.tensor(\n",
    "        [[train_node_map[idx.item()] for idx in train_edge_index[0]],\n",
    "         [train_node_map[idx.item()] for idx in train_edge_index[1]]],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "\n",
    "    # Test용 edge_index와 노드 인덱스 재매핑\n",
    "    test_edge_index = graph_data.edge_index[:, test_edge_mask]\n",
    "    test_node_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(test_nodes)}\n",
    "    test_edge_index = torch.tensor(\n",
    "        [[test_node_map[idx.item()] for idx in test_edge_index[0]],\n",
    "         [test_node_map[idx.item()] for idx in test_edge_index[1]]],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "\n",
    "    # Train 데이터 생성\n",
    "    train_data = Data(\n",
    "        x=graph_data.x[train_mask],\n",
    "        y=graph_data.y[train_mask],\n",
    "        edge_index=train_edge_index\n",
    "    )\n",
    "\n",
    "    # Test 데이터 생성\n",
    "    test_data = Data(\n",
    "        x=graph_data.x[test_mask],\n",
    "        y=graph_data.y[test_mask],\n",
    "        edge_index=test_edge_index\n",
    "    )\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Train Nodes: {train_data.x.shape[0]}, Train Edges: {train_data.edge_index.shape[1]}\")\n",
    "    print(f\"Train edge_index 최대값: {train_data.edge_index.max().item()}\")\n",
    "    print(f\"Test Nodes: {test_data.x.shape[0]}, Test Edges: {test_data.edge_index.shape[1]}\")\n",
    "    print(f\"Test edge_index 최대값: {test_data.edge_index.max().item()}\")\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# 예시 사용\n",
    "# graph_data = Data(x=torch.randn(2071, 6), edge_index=torch.randint(0, 2071, (2, 8644)), y=torch.randn(2071, 1))\n",
    "# train_data, test_data = split_graph_data(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train-Test Dataset --------------------\n",
      "Train Nodes: 2588, Train Edges: 8632\n",
      "Train edge_index 최대값: 2587\n",
      "Test Nodes: 646, Test Edges: 1030\n",
      "Test edge_index 최대값: 645\n",
      "-------------------- Calibration Dataset --------------------\n",
      "Train data: 2071 nodes, 5913 edges\n",
      "Train edge_index 최대값: 2070\n",
      "Calibration data: 517 nodes, 765 edges\n",
      "Calibration edge_index 최대값: 516\n",
      "Test data: 646 nodes, 1030 edges\n",
      "Test edge_index 최대값: 645\n"
     ]
    }
   ],
   "source": [
    "print('-' * 20, 'Train-Test Dataset', '-' * 20)\n",
    "train_data, test_data = split_graph_data(graph_data_county_edu)\n",
    "\n",
    "print('-' * 20, 'Calibration Dataset', '-' * 20)\n",
    "# cp_train_data, calibration_data = split_graph_data(train_data)\n",
    "num_train = train_data.x.shape[0]\n",
    "num_calibration = int(num_train * 0.2)  # Calibration 비율 20%\n",
    "\n",
    "# 랜덤하게 인덱스 생성\n",
    "indices = torch.randperm(num_train)\n",
    "calibration_indices = indices[:num_calibration]\n",
    "cp_train_indices = indices[num_calibration:]\n",
    "\n",
    "# 노드 마스크 생성\n",
    "cp_train_mask = torch.zeros(num_train, dtype=torch.bool)\n",
    "calibration_mask = torch.zeros(num_train, dtype=torch.bool)\n",
    "cp_train_mask[cp_train_indices] = True\n",
    "calibration_mask[calibration_indices] = True\n",
    "\n",
    "# cp_train용 edge_index 필터링 및 재매핑\n",
    "cp_train_edge_mask = cp_train_mask[train_data.edge_index[0]] & cp_train_mask[train_data.edge_index[1]]\n",
    "cp_train_edge_index = train_data.edge_index[:, cp_train_edge_mask]\n",
    "cp_train_node_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(cp_train_indices)}\n",
    "cp_train_edge_index = torch.tensor(\n",
    "    [[cp_train_node_map[idx.item()] for idx in cp_train_edge_index[0]],\n",
    "     [cp_train_node_map[idx.item()] for idx in cp_train_edge_index[1]]],\n",
    "    dtype=torch.long\n",
    ")\n",
    "\n",
    "# calibration용 edge_index 필터링 및 재매핑\n",
    "calibration_edge_mask = calibration_mask[train_data.edge_index[0]] & calibration_mask[train_data.edge_index[1]]\n",
    "calibration_edge_index = train_data.edge_index[:, calibration_edge_mask]\n",
    "calibration_node_map = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(calibration_indices)}\n",
    "calibration_edge_index = torch.tensor(\n",
    "    [[calibration_node_map[idx.item()] for idx in calibration_edge_index[0]],\n",
    "     [calibration_node_map[idx.item()] for idx in calibration_edge_index[1]]],\n",
    "    dtype=torch.long\n",
    ")\n",
    "\n",
    "# cp_train_data 생성\n",
    "cp_train_data = Data(\n",
    "    x=train_data.x[cp_train_indices],\n",
    "    y=train_data.y[cp_train_indices],\n",
    "    edge_index=cp_train_edge_index\n",
    ")\n",
    "\n",
    "# calibration_data 생성\n",
    "calibration_data = Data(\n",
    "    x=train_data.x[calibration_indices],\n",
    "    y=train_data.y[calibration_indices],\n",
    "    edge_index=calibration_edge_index\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Train data: {cp_train_data.x.shape[0]} nodes, {cp_train_data.edge_index.shape[1]} edges\")\n",
    "print(f\"Train edge_index 최대값: {cp_train_data.edge_index.max().item()}\")\n",
    "print(f\"Calibration data: {calibration_data.x.shape[0]} nodes, {calibration_data.edge_index.shape[1]} edges\")\n",
    "print(f\"Calibration edge_index 최대값: {calibration_data.edge_index.max().item()}\")\n",
    "print(f\"Test data: {test_data.x.shape[0]} nodes, {test_data.edge_index.shape[1]} edges\")\n",
    "print(f\"Test edge_index 최대값: {test_data.edge_index.max().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQR-GNN\n",
    "in_dim = train_data.x.shape[1]\n",
    "hidden_dim = 64\n",
    "learning_rate = 1e-3\n",
    "weight = 1e-3\n",
    "num_epochs = 500\n",
    "\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "model = GQNN_R(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "criterion = QRLoss()\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    taus = torch.rand(train_data.x.size(0), 1, dtype=torch.float32, device=device)\n",
    "    preds = model(train_data.x, train_data.edge_index, taus)\n",
    "    loss = criterion(preds, train_data.y, taus)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epochs.append(epoch+1)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # print(f\"Epoch {epoch}: Quantile Loss = {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(epochs, losses, color ='blue')\n",
    "plt.title('SQR-GNN Loss', size=10)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "test_data = test_data.to(device)\n",
    "tau_low = 0.05\n",
    "tau_upper = 0.95\n",
    "\n",
    "tau_lows = torch.full((train_data.x.size(0), 1), tau_low, dtype=torch.float32, device=device)\n",
    "tau_uppers = torch.full((train_data.x.size(0), 1), tau_upper, dtype=torch.float32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_low_preds = model(train_data.x, train_data.edge_index, tau_lows).cpu().numpy()\n",
    "    train_upper_preds = model(train_data.x, train_data.edge_index, tau_uppers).cpu().numpy()\n",
    "    train_targets = train_data.y.cpu().numpy()\n",
    "evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=0.9)\n",
    "\n",
    "pastel_colors = sns.color_palette('Dark2')\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(train_data.x, train_data.y, train_low_preds, train_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 3))  \n",
    "fig.suptitle('SQR-GNN Train', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[0], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), train_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), train_low_preds.squeeze(), train_upper_preds.squeeze(), color=pastel_colors[0], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()\n",
    "\n",
    "tau_lows = torch.full((test_data.x.size(0), 1), tau_low, dtype=torch.float32, device=device)\n",
    "tau_uppers = torch.full((test_data.x.size(0), 1), tau_upper, dtype=torch.float32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_low_preds = model(test_data.x, test_data.edge_index, tau_lows).cpu().numpy()\n",
    "    test_upper_preds = model(test_data.x, test_data.edge_index, tau_uppers).cpu().numpy()\n",
    "    test_targets = test_data.y.cpu().numpy()\n",
    "evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=0.9)\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(test_data.x, test_data.y, test_low_preds, test_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 3))  \n",
    "fig.suptitle('SQR-GNN Test', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[1], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), test_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), test_low_preds.squeeze(), test_upper_preds.squeeze(), color=pastel_colors[1], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQR-GNN\n",
    "in_dim = train_data.x.shape[1]\n",
    "hidden_dim = 64\n",
    "learning_rate = 1e-3\n",
    "weight = 1e-3\n",
    "num_epochs = 500\n",
    "target=0.9\n",
    "lambda_factor=1\n",
    "\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "model = GQNN_N(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "criterion = RQRLoss(target=target, lambda_factor=lambda_factor)\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = model(train_data.x, train_data.edge_index)\n",
    "    loss = criterion(preds, train_data.y)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epochs.append(epoch+1)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # print(f\"Epoch {epoch}: Quantile Loss = {loss.item():.4f}\")\n",
    "    \n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(epochs, losses, color ='red')\n",
    "plt.title('RQR-GNN Loss', size=10)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_preds = model(train_data.x, train_data.edge_index)\n",
    "    train_low_preds = train_preds[:, 0].cpu().numpy()\n",
    "    train_upper_preds = train_preds[:, 1].cpu().numpy()\n",
    "    train_targets = train_data.y.cpu().numpy()\n",
    "evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=0.9)\n",
    "\n",
    "pastel_colors = sns.color_palette('Dark2')\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(train_data.x, train_data.y, train_low_preds, train_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 3))  \n",
    "fig.suptitle('RQR-GNN Train', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[2], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), train_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), train_low_preds.squeeze(), train_upper_preds.squeeze(), color=pastel_colors[2], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = model(test_data.x, test_data.edge_index)\n",
    "    test_low_preds = test_preds[:, 0].cpu().numpy()\n",
    "    test_upper_preds = test_preds[:, 1].cpu().numpy()\n",
    "    test_targets = test_data.y.cpu().numpy()\n",
    "evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=0.9)\n",
    "\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(test_data.x, test_data.y, test_low_preds, test_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 3))  \n",
    "fig.suptitle('RQR-GNN Test', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[3], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), test_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), test_low_preds.squeeze(), test_upper_preds.squeeze(), color=pastel_colors[3], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15/500 [00:04<02:37,  3.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcp_train_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcp_train_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(preds, cp_train_data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 61\u001b[0m, in \u001b[0;36mGNN_CP.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[0;32m---> 61\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     62\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index))\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch_geometric/nn/conv/sage_conv.py:139\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    137\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_weight \u001b[38;5;129;01mand\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_r\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_r\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n\u001b[1;32m    142\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(out, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch_geometric/nn/dense/linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CP-GNN\n",
    "in_dim = train_data.x.shape[1]\n",
    "hidden_dim = 64\n",
    "learning_rate = 1e-3\n",
    "weight = 1e-3\n",
    "num_epochs = 500\n",
    "alpha = 0.1  # 신뢰수준 90% (1 - alpha)\n",
    "\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "# device =torch.device(\"cuda\")\n",
    "cp_train_data = cp_train_data.to(device)\n",
    "\n",
    "model = GNN_CP(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = model(cp_train_data.x, cp_train_data.edge_index)\n",
    "    loss = F.mse_loss(preds, cp_train_data.y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epochs.append(epoch+1)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(epochs, losses, color ='yellow')\n",
    "plt.title('CP-GNN Loss', size=10)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "calibration_data = calibration_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_cal = model(calibration_data.x, calibration_data.edge_index)\n",
    "    preds_train = model(train_data.x, train_data.edge_index).cpu().numpy()\n",
    "    preds_test = model(test_data.x, test_data.edge_index).cpu().numpy()\n",
    "\n",
    "conformal_scores = torch.abs(calibration_data.y- preds_cal).cpu().numpy()\n",
    "q_hat = np.quantile(conformal_scores, 1 - alpha)\n",
    "\n",
    "train_low_preds = preds_train - q_hat\n",
    "train_upper_preds = preds_train + q_hat\n",
    "train_targets = train_data.y.cpu().numpy()\n",
    "evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=0.9)\n",
    "\n",
    "pastel_colors = sns.color_palette('Dark2')\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(train_data.x, train_data.y, train_low_preds, train_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 3))  \n",
    "fig.suptitle('CP-GNN Train', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[6], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), train_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), train_low_preds.squeeze(), train_upper_preds.squeeze(), color=pastel_colors[6], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()\n",
    "\n",
    "test_low_preds = preds_test - q_hat\n",
    "test_upper_preds = preds_test + q_hat\n",
    "test_targets = test_data.y.cpu().numpy()\n",
    "evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=0.9)\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(test_data.x, test_data.y, test_low_preds, test_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 3))  \n",
    "fig.suptitle('CP-GNN Test', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[7], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), test_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), test_low_preds.squeeze(), test_upper_preds.squeeze(), color=pastel_colors[7], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGQNN_D\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight)\n\u001b[1;32m     14\u001b[0m criterion \u001b[38;5;241m=\u001b[39m IQRLoss(target_coverage\u001b[38;5;241m=\u001b[39mtarget)\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/gqnn/lib/python3.11/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# DGQNN\n",
    "in_dim = train_data.x.shape[1]\n",
    "hidden_dim = 64\n",
    "learning_rate = 1e-3\n",
    "weight = 1e-3\n",
    "num_epochs = 500\n",
    "target=0.9\n",
    "\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "model = GQNN_D(in_dim=in_dim, hidden_dim=hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "criterion = IQRLoss(target_coverage=target)\n",
    "\n",
    "epochs = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds_low, preds_upper, taus_low, taus_upper = model(train_data.x, train_data.edge_index)\n",
    "    loss = criterion(preds_low, preds_upper, taus_low, taus_upper, train_data.y)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epochs.append(epoch+1)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # print(f\"Epoch {epoch}: Quantile Loss = {loss.item():.4f}\")\n",
    "    \n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(epochs, losses, color ='green')\n",
    "plt.title('DGQNN Loss', size=10)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_low, preds_upper, _, _ = model(train_data.x, train_data.edge_index)\n",
    "    train_low_preds = preds_low.cpu().numpy()\n",
    "    train_upper_preds = preds_upper.cpu().numpy()\n",
    "    train_targets = train_data.y.cpu().numpy()\n",
    "evaluate_model_performance(train_low_preds, train_upper_preds, train_targets, target=0.9)\n",
    "\n",
    "pastel_colors = sns.color_palette('Dark2')\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(train_data.x, train_data.y, train_low_preds, train_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 3))  \n",
    "fig.suptitle('DGQNN Train', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[4], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), train_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), train_low_preds.squeeze(), train_upper_preds.squeeze(), color=pastel_colors[4], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds_low, preds_upper, _, _ = model(test_data.x, test_data.edge_index)\n",
    "    test_low_preds = preds_low.cpu().numpy()\n",
    "    test_upper_preds = preds_upper.cpu().numpy()\n",
    "    test_targets = test_data.y.cpu().numpy()\n",
    "evaluate_model_performance(test_low_preds, test_upper_preds, test_targets, target=0.9)\n",
    "\n",
    "x_st, y_st, (low_r_st, upper_r_st) = sort_by_y(test_data.x, test_data.y, test_low_preds, test_upper_preds)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 3))  \n",
    "fig.suptitle('DGQNN Test', size=10)\n",
    "\n",
    "axes[0].scatter(range(len(x_st)), y_st, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[0].fill_between(range(len(x_st)), low_r_st, upper_r_st, color=pastel_colors[5], alpha=0.5)\n",
    "axes[0].set_xlabel(\"Sotred Node Index\") \n",
    "axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "axes[1].scatter(range(len(x_st)), test_targets, alpha=0.3, color='blue', label=\"True Values\", s=15)\n",
    "axes[1].fill_between(range(len(x_st)), test_low_preds.squeeze(), test_upper_preds.squeeze(), color=pastel_colors[5], alpha=0.5)\n",
    "axes[1].set_xlabel(\"Node Index\")  \n",
    "axes[1].set_ylabel(\"Values\") \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gqnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
