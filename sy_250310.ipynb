{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch_geometric.transforms import RandomNodeSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_data(num_nodes=1000):\n",
    "    np.random.seed(1127)\n",
    "    torch.manual_seed(1127)\n",
    "    \n",
    "    # 비선형 데이터\n",
    "    X = np.random.rand(num_nodes, 5)  # 5차원\n",
    "    y = np.sin(X[:, 0] * 3) + 0.1 * np.random.rand(num_nodes)\n",
    "    \n",
    "    edge_index = torch.randint(0, num_nodes, (2, num_nodes * 2))\n",
    "    \n",
    "    # PyG 데이터 변환\n",
    "    return Data(\n",
    "        x = torch.tensor(X, dtype = torch.float32),\n",
    "        edge_index= edge_index,\n",
    "        y = torch.tensor(y, dtype = torch.float32).unsqueeze(1),\n",
    "    )\n",
    "\n",
    "def generate_noisy_graph_data(num_nodes=1000, noise_type=\"gaussian\", noise_level=0.1, outlier_ratio=0.05):\n",
    "    \"\"\"\n",
    "    다양한 노이즈를 추가하여 그래프 데이터를 생성하는 함수\n",
    "\n",
    "    Args:\n",
    "    - num_nodes (int): 노드 개수\n",
    "    - noise_type (str): 추가할 노이즈 유형 (\"gaussian\", \"uniform\", \"outlier\", \"edge_noise\")\n",
    "    - noise_level (float): 노이즈의 강도 (가우시안 및 유니폼 노이즈)\n",
    "    - outlier_ratio (float): 이상치(outlier) 비율\n",
    "\n",
    "    Returns:\n",
    "    - PyG Data 객체\n",
    "    \"\"\"\n",
    "    np.random.seed(1127)\n",
    "    torch.manual_seed(1127)\n",
    "    \n",
    "    X = np.random.rand(num_nodes, 5)  # 5차원 특징\n",
    "    y = np.sin(X[:, 0] * 3) + 0.1 * np.random.rand(num_nodes)  # 기본 타겟\n",
    "    \n",
    "    if noise_type == \"gaussian\":\n",
    "        y += np.random.normal(0, noise_level, size=num_nodes)\n",
    "    elif noise_type == \"uniform\":\n",
    "        y += np.random.uniform(-noise_level, noise_level, size=num_nodes)\n",
    "    elif noise_type == \"outlier\":\n",
    "        num_outliers = int(num_nodes * outlier_ratio)\n",
    "        outlier_indices = np.random.choice(num_nodes, num_outliers, replace=False)\n",
    "        y[outlier_indices] += np.random.normal(3, 1.0, size=num_outliers)  # 극단적인 변화\n",
    "\n",
    "    # 그래프 구조적 노이즈 (엣지 변경)\n",
    "    edge_index = torch.randint(0, num_nodes, (2, num_nodes * 2))\n",
    "    if noise_type == \"edge_noise\":\n",
    "        # 엣지에 무작위 잡음을 추가하여 구조적 변형 수행\n",
    "        num_noisy_edges = int(edge_index.shape[1] * noise_level)\n",
    "        noise_indices = np.random.choice(edge_index.shape[1], num_noisy_edges, replace=False)\n",
    "        edge_index[:, noise_indices] = torch.randint(0, num_nodes, (2, num_noisy_edges))\n",
    "\n",
    "    return Data(\n",
    "        x=torch.tensor(X, dtype=torch.float32),\n",
    "        edge_index=edge_index,\n",
    "        y=torch.tensor(y, dtype=torch.float32).unsqueeze(1),\n",
    "    )\n",
    " \n",
    "def load_graph_data(dataset, category):\n",
    "    edge_path = f'dataset/{dataset}/{category}/musae_{category}_edges.csv'\n",
    "    feature_path = f'dataset/{dataset}/{category}/musae_{category}_features.json'\n",
    "    target_path = f'dataset/{dataset}/{category}/musae_{category}_target.csv'\n",
    "    \n",
    "    # 엣지 데이터 로드\n",
    "    edge_df = pd.read_csv(edge_path)\n",
    "    edge_index = torch.tensor(edge_df.values.T, dtype=torch.long)\n",
    "    \n",
    "    # 피처 데이터 로드\n",
    "    with open(feature_path, \"r\") as f:\n",
    "        features_dict = json.load(f)\n",
    "    \n",
    "    node_ids = sorted(map(int, features_dict.keys()))  # 노드 ID 정렬\n",
    "    node_id_map = {old_id: new_id for new_id, old_id in enumerate(node_ids)}\n",
    "    \n",
    "    num_nodes = len(node_ids)\n",
    "    num_features = max(max(v) for v in features_dict.values()) + 1  # 가장 큰 feature index 찾기\n",
    "    x = torch.zeros((num_nodes, num_features), dtype=torch.float32)\n",
    "    \n",
    "    for node, features in features_dict.items():\n",
    "        new_id = node_id_map[int(node)]  # 노드 ID 변환\n",
    "        x[new_id, features] = 1.0  # One-hot 인코딩\n",
    "    \n",
    "    # 타겟 데이터 로드\n",
    "    target_df = pd.read_csv(target_path)\n",
    "    target_df[\"id\"] = target_df[\"id\"].map(node_id_map)  # 노드 ID 변환\n",
    "    target_df = target_df.dropna().astype(int)  # 변환되지 않은 노드 제거\n",
    "    \n",
    "    # y = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    y = torch.zeros((num_nodes, 1), dtype=torch.long)  # [, 1] 형태로 변경\n",
    "    if dataset == 'wiki':\n",
    "        y[target_df[\"id\"].values] = torch.tensor(target_df[\"target\"].values, dtype=torch.long).view(-1, 1)\n",
    "    elif dataset == 'twitch':\n",
    "        y[target_df[\"id\"].values] = torch.tensor(target_df[\"views\"].values, dtype=torch.long).view(-1, 1)\n",
    "    \n",
    "    # PyG Data 객체 생성\n",
    "    graph_data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    return graph_data\n",
    "\n",
    "def max_normalize(x):\n",
    "    return x / np.max(np.abs(x)) if np.max(np.abs(x)) != 0 else x\n",
    "\n",
    "def std_normalize(x):\n",
    "    return (x - np.mean(x)) / np.std(x) if np.std(x) != 0 else np.zeros(len(x))\n",
    "\n",
    "def int_normalize(x):\n",
    "    return ((x - np.min(x)) / (np.max(x) - np.min(x)) * 2 - 1) if np.std(x) != 0 else np.zeros(len(x))\n",
    "\n",
    "def simulate_ising(n, h0, J):\n",
    "    G = nx.grid_2d_graph(n, n)\n",
    "    l = np.linspace(-1.0, 1.0, n)\n",
    "    \n",
    "    s = np.random.choice([-1, 1], size=(n, n))\n",
    "    # Placeholder for metropolis algorithm\n",
    "    y = s.flatten()\n",
    "    f = [[l[i], l[j]] for j in range(n) for i in range(n)]\n",
    "    \n",
    "    return G, [nx.to_scipy_sparse_matrix(G)], y, f\n",
    "\n",
    "def parse_mean_fill(series, normalize=False):\n",
    "    series = series.replace({',': ''}, regex=True)\n",
    "    series = pd.to_numeric(series, errors='coerce')\n",
    "    mean_val = series.mean()\n",
    "    series.fillna(mean_val, inplace=True)\n",
    "    \n",
    "    if normalize:\n",
    "        series = (series - mean_val) / series.std()\n",
    "    \n",
    "    return series.values\n",
    "\n",
    "def read_county(prediction, year):\n",
    "    adj = pd.read_csv(\"dataset/election/adjacency.txt\", header=None, sep=\"\\t\", dtype=str, encoding=\"ISO-8859-1\")\n",
    "    fips2cty = {row[1]: row[0] for _, row in adj.iterrows() if pd.notna(row[1])}\n",
    "    \n",
    "    hh = adj.iloc[:, 1].ffill().astype(int)\n",
    "    tt = adj.iloc[:, 3].astype(int)\n",
    "    \n",
    "    fips = sorted(set(hh).union(set(tt)))\n",
    "    id2num = {id_: num for num, id_ in enumerate(fips)}\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(len(id2num)))\n",
    "    G.add_edges_from([(id2num[h], id2num[t]) for h, t in zip(hh, tt)])\n",
    "    \n",
    "    # Load datasets\n",
    "    VOT = pd.read_csv(\"dataset/election/election.csv\")\n",
    "    ICM = pd.read_csv(\"dataset/election/income.csv\")\n",
    "    POP = pd.read_csv(\"dataset/election/population.csv\")\n",
    "    EDU = pd.read_csv(\"dataset/election/education.csv\")\n",
    "    UEP = pd.read_csv(\"dataset/election/unemployment.csv\")\n",
    "    \n",
    "    cty = pd.DataFrame({'FIPS': fips, 'County': [fips2cty.get(f, '') for f in fips]})\n",
    "    vot = VOT[['fips_code', f'dem_{year}', f'gop_{year}']].rename(columns={'fips_code': 'FIPS'})\n",
    "    icm = ICM[['FIPS', f'MedianIncome{min(max(2011, year), 2018)}']]\n",
    "    pop = POP[['FIPS', f'R_NET_MIG_{min(max(2011, year), 2018)}', f'R_birth_{min(max(2011, year), 2018)}', f'R_death_{min(max(2011, year), 2018)}']]\n",
    "    edu = EDU[['FIPS', f'BachelorRate{year}']]\n",
    "    uep = UEP[['FIPS', f'Unemployment_rate_{min(max(2007, year), 2018)}']]\n",
    "    \n",
    "    dat = cty.merge(vot, on='FIPS', how='left')\n",
    "    dat = dat.merge(icm, on='FIPS', how='left')\n",
    "    dat = dat.merge(pop, on='FIPS', how='left')\n",
    "    dat = dat.merge(edu, on='FIPS', how='left')\n",
    "    dat = dat.merge(uep, on='FIPS', how='left')\n",
    "    \n",
    "    # Extract features and labels\n",
    "    dem = parse_mean_fill(dat.iloc[:, 2])\n",
    "    gop = parse_mean_fill(dat.iloc[:, 3])\n",
    "    \n",
    "    ff = np.zeros((len(dat), 7), dtype=np.float32)\n",
    "    for i in range(6):\n",
    "        ff[:, i] = parse_mean_fill(dat.iloc[:, i + 4], normalize=True)\n",
    "    \n",
    "    ff[:, 6] = (gop - dem) / (gop + dem)\n",
    "    \n",
    "    label_mapping = {\n",
    "        \"income\": 0, \"migration\": 1, \"birth\": 2, \"death\": 3,\n",
    "        \"education\": 4, \"unemployment\": 5, \"election\": 6\n",
    "    }\n",
    "    \n",
    "    if prediction not in label_mapping:\n",
    "        raise ValueError(\"Unexpected prediction type\")\n",
    "    \n",
    "    pos = label_mapping[prediction]\n",
    "    y = ff[:, pos]\n",
    "    f = [np.concatenate((ff[i, :pos], ff[i, pos + 1:])) for i in range(len(dat))]\n",
    "    \n",
    "    return G, [csr_matrix(nx.adjacency_matrix(G))], y, f\n",
    "\n",
    "def load_county_graph_data(prediction: str, year: int):\n",
    "    G, A, labels, feats = read_county(prediction, year)\n",
    "\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    pyg_data = from_networkx(G)\n",
    "\n",
    "    edge_index = pyg_data.edge_index\n",
    "    sorted_edges = torch.sort(edge_index, dim=0)[0]  # (u, v)와 (v, u)를 정렬\n",
    "    unique_edges = torch.unique(sorted_edges, dim=1)  # 고유 엣지만 유지\n",
    "    pyg_data.edge_index = unique_edges  # 중복 제거된 edge_index 적용\n",
    "\n",
    "    pyg_data.x = torch.tensor(feats, dtype=torch.float)\n",
    "\n",
    "    pyg_data.y = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    return pyg_data\n",
    "\n",
    "def read_transportation_network(network_name, net_skips, net_cols, netf_cols, flow_skips, flow_cols, V_range):\n",
    "    # Load data\n",
    "    dat_net = pd.read_csv(f\"dataset/transportation/{network_name}/{network_name}_net.tntp\", \n",
    "                           skiprows=net_skips, sep='\\s+', usecols=net_cols, header=None).values\n",
    "    dat_netf = pd.read_csv(f\"dataset/transportation/{network_name}/{network_name}_net.tntp\", \n",
    "                            skiprows=net_skips, sep='\\s+', usecols=netf_cols, header=None).values\n",
    "    dat_flow = pd.read_csv(f\"dataset/transportation/{network_name}/{network_name}_flow.tntp\", \n",
    "                            skiprows=flow_skips, sep='\\s+', usecols=flow_cols, header=None).values\n",
    "    \n",
    "    # Map node labels to indices\n",
    "    lb2id = {v: i for i, v in enumerate(V_range, start=1)}\n",
    "    NV = len(V_range)\n",
    "    \n",
    "    # Create directed graph\n",
    "    g = nx.DiGraph()\n",
    "    g.add_nodes_from(range(1, NV + 1))\n",
    "    \n",
    "    for src, dst in dat_net:\n",
    "        if src in lb2id and dst in lb2id:\n",
    "            g.add_edge(lb2id[src], lb2id[dst])\n",
    "    \n",
    "    # Edge labels\n",
    "    flow_dict = {}\n",
    "    for src, dst, flow in dat_flow:\n",
    "        if src in lb2id and dst in lb2id:\n",
    "            flow_dict[(lb2id[src], lb2id[dst])] = flow\n",
    "    \n",
    "    y = np.array([flow_dict.get((e[0], e[1]), 0) for e in g.edges()])\n",
    "    y = (y - np.mean(y)) / np.std(y)  # Standard normalization\n",
    "    \n",
    "    # Edge features\n",
    "    netf_dict = {}\n",
    "    for i in range(len(dat_net)):\n",
    "        src, dst = dat_net[i]\n",
    "        if src in lb2id and dst in lb2id:\n",
    "            netf_dict[(lb2id[src], lb2id[dst])] = dat_netf[i]\n",
    "    \n",
    "    ff = np.array([netf_dict[e] for e in g.edges()])\n",
    "    mean_ff = np.mean(ff, axis=0)\n",
    "    std_ff = np.std(ff, axis=0)\n",
    "    std_ff[std_ff == 0] = 1  # Prevent division by zero\n",
    "    netf = (ff - mean_ff) / std_ff  \n",
    "    \n",
    "    f = list(netf)\n",
    "    \n",
    "    # Line graph transformation\n",
    "    G1 = nx.Graph()\n",
    "    G2 = nx.Graph()\n",
    "    sorted_edges = sorted(g.edges())\n",
    "    tuple2id = {e: i for i, e in enumerate(sorted_edges)}\n",
    "    \n",
    "    for u in g.nodes:\n",
    "        innbrs = list(g.predecessors(u))\n",
    "        outnbrs = list(g.successors(u))\n",
    "        \n",
    "        for v in innbrs:\n",
    "            for w in outnbrs:\n",
    "                if (v, u) in tuple2id and (u, w) in tuple2id:\n",
    "                    G1.add_edge(tuple2id[(v, u)], tuple2id[(u, w)])\n",
    "        \n",
    "        for v in innbrs:\n",
    "            for w in innbrs:\n",
    "                if w > v and (v, u) in tuple2id and (w, u) in tuple2id:\n",
    "                    G2.add_edge(tuple2id[(v, u)], tuple2id[(w, u)])\n",
    "        \n",
    "        for v in outnbrs:\n",
    "            for w in outnbrs:\n",
    "                if w > v and (u, v) in tuple2id and (u, w) in tuple2id:\n",
    "                    G2.add_edge(tuple2id[(u, v)], tuple2id[(u, w)])\n",
    "                    \n",
    "    size = max(len(G1.nodes), len(G2.nodes))\n",
    "    A1 = np.zeros((size, size))\n",
    "    A2 = np.zeros((size, size))\n",
    "    \n",
    "    A1[:nx.number_of_nodes(G1), :nx.number_of_nodes(G1)] = nx.adjacency_matrix(G1).todense()\n",
    "    A2[:nx.number_of_nodes(G2), :nx.number_of_nodes(G2)] = nx.adjacency_matrix(G2).todense()\n",
    "    \n",
    "    A = A1 + A2\n",
    "    \n",
    "    return nx.Graph(A), A, y, f\n",
    "\n",
    "def load_trans_graph_data(city: str):\n",
    "    if city == 'Anaheim':\n",
    "        G, A, labels, feats = read_transportation_network(city, 8, [0, 1], [2, 3, 4, 7], 6, [0, 1, 3], range(1, 417))\n",
    "    elif city == 'ChicagoSketch':\n",
    "        G, A, labels, feats = read_transportation_network(city, 7, [0, 1], [2, 3, 4, 7], 1, [0, 1, 2], range(388, 934))\n",
    "\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    pyg_data = from_networkx(G)\n",
    "\n",
    "    edge_index = pyg_data.edge_index\n",
    "    sorted_edges = torch.sort(edge_index, dim=0)[0]  # (u, v)와 (v, u)를 정렬\n",
    "    unique_edges = torch.unique(sorted_edges, dim=1)  # 고유 엣지만 유지\n",
    "    pyg_data.edge_index = unique_edges  # 중복 제거된 edge_index 적용\n",
    "\n",
    "    pyg_data.x = torch.tensor(feats, dtype=torch.float)\n",
    "\n",
    "    pyg_data.y = torch.tensor(labels, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    return pyg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_419030/223594441.py:202: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  pyg_data.x = torch.tensor(feats, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# 기본 비선형 그래프 데이터\n",
    "graph_data_basic = generate_graph_data(num_nodes=1000)\n",
    "\n",
    "# 노이즈 비선형 그래프 데이터\n",
    "graph_data_noise_gaussian = generate_noisy_graph_data(num_nodes=1000, noise_type='gaussian', noise_level=0.2)\n",
    "graph_data_noise_uniform = generate_noisy_graph_data(num_nodes=1000, noise_type='uniform', noise_level=0.2)\n",
    "graph_data_noise_outlier = generate_noisy_graph_data(num_nodes=1000, noise_type='outlier', noise_level=0.2)\n",
    "graph_data_noise_edge = generate_noisy_graph_data(num_nodes=1000, noise_type='edge_noise', noise_level=0.2)\n",
    "\n",
    "# County 데이터셋\n",
    "graph_data_county_edu = load_county_graph_data('education', 2012)\n",
    "graph_data_county_elec = load_county_graph_data('election', 2012)\n",
    "graph_data_county_inc = load_county_graph_data('income', 2012)\n",
    "graph_data_county_unemp = load_county_graph_data('unemployment', 2012)\n",
    "\n",
    "# Twitch 데이터셋\n",
    "graph_data_twitch_de = load_graph_data('twitch', 'DE')\n",
    "graph_data_twitch_engb = load_graph_data('twitch', 'ENGB')\n",
    "graph_data_twitch_es = load_graph_data('twitch', 'ES')\n",
    "graph_data_twitch_fr = load_graph_data('twitch', 'FR')\n",
    "graph_data_twitch_ptbr = load_graph_data('twitch', 'PTBR')\n",
    "graph_data_twitch_ru = load_graph_data('twitch', 'RU')\n",
    "\n",
    "# Wikipedia 데이터\n",
    "graph_data_wiki_ch = load_graph_data('wikipedia', 'chameleon')\n",
    "graph_data_wiki_cr = load_graph_data('wikipedia', 'crocodile')\n",
    "graph_data_wiki_sq = load_graph_data('wikipedia', 'squirrel')\n",
    "\n",
    "# Transfortation 데이터셋\n",
    "graph_data_trans_ana = load_trans_graph_data('Anaheim')\n",
    "graph_data_trans_chica = load_trans_graph_data('ChicagoSketch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Nodes</th>\n",
       "      <th>Edges</th>\n",
       "      <th>Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Noise_Gaussian</td>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Noise_Uniform</td>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Noise_Outlier</td>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Noise_Edge</td>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>County_Education</td>\n",
       "      <td>3234</td>\n",
       "      <td>12717</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>County_Election</td>\n",
       "      <td>3234</td>\n",
       "      <td>12717</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>County_Income</td>\n",
       "      <td>3234</td>\n",
       "      <td>12717</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>County_Unemployment</td>\n",
       "      <td>3234</td>\n",
       "      <td>12717</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Twitch_DE</td>\n",
       "      <td>9498</td>\n",
       "      <td>153138</td>\n",
       "      <td>3170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Twitch_ENGB</td>\n",
       "      <td>7126</td>\n",
       "      <td>35324</td>\n",
       "      <td>3170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Twitch_ES</td>\n",
       "      <td>4648</td>\n",
       "      <td>59382</td>\n",
       "      <td>3170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Twitch_FR</td>\n",
       "      <td>6549</td>\n",
       "      <td>112666</td>\n",
       "      <td>3170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Twitch_PTBR</td>\n",
       "      <td>1912</td>\n",
       "      <td>31299</td>\n",
       "      <td>3169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Twitch_RU</td>\n",
       "      <td>4385</td>\n",
       "      <td>37304</td>\n",
       "      <td>3170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wikipedia_Chameleon</td>\n",
       "      <td>2277</td>\n",
       "      <td>36101</td>\n",
       "      <td>3132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wikipedia_Crocodile</td>\n",
       "      <td>11631</td>\n",
       "      <td>180020</td>\n",
       "      <td>13183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Wikipedia_Squirrel</td>\n",
       "      <td>5201</td>\n",
       "      <td>217073</td>\n",
       "      <td>3148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Transportation_Anaheim</td>\n",
       "      <td>914</td>\n",
       "      <td>3638</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Transportation_Chicago</td>\n",
       "      <td>2176</td>\n",
       "      <td>14961</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Dataset  Nodes   Edges  Features\n",
       "0                    Basic   1000    2000         5\n",
       "1           Noise_Gaussian   1000    2000         5\n",
       "2            Noise_Uniform   1000    2000         5\n",
       "3            Noise_Outlier   1000    2000         5\n",
       "4               Noise_Edge   1000    2000         5\n",
       "5         County_Education   3234   12717         6\n",
       "6          County_Election   3234   12717         6\n",
       "7            County_Income   3234   12717         6\n",
       "8      County_Unemployment   3234   12717         6\n",
       "9                Twitch_DE   9498  153138      3170\n",
       "10             Twitch_ENGB   7126   35324      3170\n",
       "11               Twitch_ES   4648   59382      3170\n",
       "12               Twitch_FR   6549  112666      3170\n",
       "13             Twitch_PTBR   1912   31299      3169\n",
       "14               Twitch_RU   4385   37304      3170\n",
       "15     Wikipedia_Chameleon   2277   36101      3132\n",
       "16     Wikipedia_Crocodile  11631  180020     13183\n",
       "17      Wikipedia_Squirrel   5201  217073      3148\n",
       "18  Transportation_Anaheim    914    3638         4\n",
       "19  Transportation_Chicago   2176   14961         4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그래프 데이터 리스트 \n",
    "graph_datasets = {\n",
    "    \"Basic\": graph_data_basic,\n",
    "    \"Noise_Gaussian\": graph_data_noise_gaussian,\n",
    "    \"Noise_Uniform\": graph_data_noise_uniform,\n",
    "    \"Noise_Outlier\": graph_data_noise_outlier,\n",
    "    \"Noise_Edge\": graph_data_noise_edge,\n",
    "    \"County_Education\": graph_data_county_edu,\n",
    "    \"County_Election\": graph_data_county_elec,\n",
    "    \"County_Income\": graph_data_county_inc,\n",
    "    \"County_Unemployment\": graph_data_county_unemp,\n",
    "    \"Twitch_DE\": graph_data_twitch_de,\n",
    "    \"Twitch_ENGB\": graph_data_twitch_engb,\n",
    "    \"Twitch_ES\": graph_data_twitch_es,\n",
    "    \"Twitch_FR\": graph_data_twitch_fr,\n",
    "    \"Twitch_PTBR\": graph_data_twitch_ptbr,\n",
    "    \"Twitch_RU\": graph_data_twitch_ru,\n",
    "    \"Wikipedia_Chameleon\": graph_data_wiki_ch,\n",
    "    \"Wikipedia_Crocodile\": graph_data_wiki_cr,\n",
    "    \"Wikipedia_Squirrel\": graph_data_wiki_sq,\n",
    "    \"Transportation_Anaheim\": graph_data_trans_ana,\n",
    "    \"Transportation_Chicago\": graph_data_trans_chica,\n",
    "}\n",
    "\n",
    "# 데이터 저장 리스트\n",
    "graph_info_list = []\n",
    "\n",
    "for dataset_name, graph_data in graph_datasets.items():\n",
    "    if graph_data is None:\n",
    "        continue  # 그래프 데이터가 없으면 스킵\n",
    "    \n",
    "    nodes = graph_data.x.shape[0] if hasattr(graph_data, \"x\") else 0  # 노드 개수\n",
    "    edges = graph_data.edge_index.shape[1] if hasattr(graph_data, \"edge_index\") else 0  # 엣지 개수\n",
    "    features = graph_data.x.shape[1] if hasattr(graph_data, \"x\") else 0  # 피처 개수\n",
    "    \n",
    "    graph_info_list.append({\"Dataset\": dataset_name, \"Nodes\": nodes, \"Edges\": edges, \"Features\": features})\n",
    "\n",
    "# 데이터 프레임 생성\n",
    "df_graph_info = pd.DataFrame(graph_info_list)\n",
    "df_graph_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_graph_data(graph_data, test_ratio=0.2):\n",
    "    transform = RandomNodeSplit(split=\"train_rest\", num_val=0.0, num_test=test_ratio)\n",
    "    data = transform(graph_data)\n",
    "\n",
    "    # Train/Test 마스크 생성\n",
    "    train_mask = data.train_mask.to(data.x.device)\n",
    "    test_mask = data.test_mask.to(data.x.device)\n",
    "\n",
    "    # 훈련 및 테스트 데이터 선택\n",
    "    train_data = data.clone()\n",
    "    test_data = data.clone()\n",
    "\n",
    "    train_data.x = data.x[train_mask]\n",
    "    train_data.y = data.y[train_mask]\n",
    "    test_data.x = data.x[test_mask]\n",
    "    test_data.y = data.y[test_mask]\n",
    "\n",
    "    # 노드 인덱스를 매핑하여 edge_index 수정 (훈련 데이터)\n",
    "    train_node_mapping = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(train_mask.nonzero(as_tuple=True)[0])}\n",
    "    train_edges_mask = train_mask[data.edge_index[0]] & train_mask[data.edge_index[1]]\n",
    "    train_edges = data.edge_index[:, train_edges_mask]\n",
    "\n",
    "    train_data.edge_index = torch.stack([\n",
    "        torch.tensor([train_node_mapping[i.item()] for i in train_edges[0]], dtype=torch.long, device=data.x.device),\n",
    "        torch.tensor([train_node_mapping[i.item()] for i in train_edges[1]], dtype=torch.long, device=data.x.device)\n",
    "    ], dim=0)\n",
    "\n",
    "    # 노드 인덱스를 매핑하여 edge_index 수정 (테스트 데이터)\n",
    "    test_node_mapping = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(test_mask.nonzero(as_tuple=True)[0])}\n",
    "    test_edges_mask = test_mask[data.edge_index[0]] & test_mask[data.edge_index[1]]\n",
    "    test_edges = data.edge_index[:, test_edges_mask]\n",
    "\n",
    "    test_data.edge_index = torch.stack([\n",
    "        torch.tensor([test_node_mapping[i.item()] for i in test_edges[0]], dtype=torch.long, device=data.x.device),\n",
    "        torch.tensor([test_node_mapping[i.item()] for i in test_edges[1]], dtype=torch.long, device=data.x.device)\n",
    "    ], dim=0)\n",
    "\n",
    "    print(f\"Train data: {train_data.x.shape[0]} nodes, {train_data.edge_index.shape[1]} edges\")\n",
    "    print(f\"Test data: {test_data.x.shape[0]} nodes, {test_data.edge_index.shape[1]} edges\")\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def augment_features(x, tau):\n",
    "    tau = tau.view(-1, 1)\n",
    "    tau_transformed = (tau - 0.5) * 12 # 분위수 값 변환: 학습 안정성 증가\n",
    "    \n",
    "    return torch.cat((x, tau_transformed.expand(x.size(0), -1)), dim = 1)\n",
    "\n",
    "def coverage_loss(y_true, y_low, y_upper, target = 0.9):\n",
    "    coverage_exp = ((y_true >= y_low) & (y_true <= y_upper)).float().mean()\n",
    "    \n",
    "    return (coverage_exp - target) ** 2\n",
    "\n",
    "def dynamic_quantile_adjustment(tau_low, tau_upper, coverage, target=0.9, learning_rate=0.005, min_gap=0.05):\n",
    "    coverage_error = coverage - target  \n",
    "    tau_adjustment = max(0.001, min(0.05, learning_rate * abs(coverage_error)))  # 과도한 조정 방지\n",
    "\n",
    "    # 분위수 업데이트\n",
    "    tau_low_new = max(0.01, tau_low - tau_adjustment if coverage > target else tau_low + tau_adjustment)\n",
    "    tau_upper_new = min(0.99, tau_upper + tau_adjustment if coverage > target else tau_upper - tau_adjustment)\n",
    "\n",
    "    # 최소 간격 유지\n",
    "    if tau_upper_new - tau_low_new < min_gap:\n",
    "        tau_mid = (tau_low_new + tau_upper_new) / 2\n",
    "        tau_low_new = max(0.01, tau_mid - min_gap / 2)\n",
    "        tau_upper_new = min(0.99, tau_mid + min_gap / 2)\n",
    "\n",
    "    return tau_low_new, tau_upper_new\n",
    "\n",
    "class GQNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels + 1, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.fc = nn.Linear(hidden_channels, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index, tau):\n",
    "        x = augment_features(x, tau)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        return self.fc(x)\n",
    "\n",
    "class QRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, y_pred, y_true, tau):\n",
    "        diff = y_true - y_pred\n",
    "        loss = torch.where(diff > 0, tau * diff, (tau - 1) * diff)\n",
    "        \n",
    "        return torch.mean(loss) \n",
    "\n",
    "class GQNN_PI(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.fc = nn.Linear(hidden_channels, 2)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # 분위수 교차 방지 (q1 ≤ q2 보장)\n",
    "        q1, q2 = torch.unbind(x, dim=1)  # (num_nodes, 2) → q1, q2로 분리\n",
    "        q1, q2 = torch.min(q1, q2), torch.max(q1, q2)  # 분위수 순서 보장\n",
    "        \n",
    "        return torch.stack([q1, q2], dim=1)\n",
    "\n",
    "class RQRLoss(nn.Module):\n",
    "    def __init__(self, target_coverage=0.9, lambda_factor=0.1):\n",
    "        super(RQRLoss, self).__init__()\n",
    "        self.target_coverage = target_coverage\n",
    "        self.lambda_factor = lambda_factor\n",
    "\n",
    "    def forward(self, q1, q2, target):\n",
    "        width = torch.relu(q2 - q1)  # 신뢰구간 너비가 음수가 되지 않도록 방지\n",
    "\n",
    "        # 분위수 회귀 손실\n",
    "        diff1 = target - q1\n",
    "        diff2 = target - q2\n",
    "        quantile_loss = torch.maximum(diff1 * diff2, torch.tensor(0.0, device=target.device))\n",
    "\n",
    "        # 신뢰구간 너비 최소화 정규화\n",
    "        width_penalty = self.lambda_factor * width ** 2 * 0.5\n",
    "\n",
    "        # Coverage Loss 추가\n",
    "        coverage = ((target >= q1) & (target <= q2)).float().mean()\n",
    "        coverage_loss = (coverage - self.target_coverage) ** 2\n",
    "\n",
    "        return torch.mean(quantile_loss + width_penalty + coverage_loss)\n",
    "\n",
    "def train_fixed_gqnn(train_data, q='all', hidden_channels=64, num_epochs=1000, learning_rate=0.001, weight=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    in_channels = train_data.x.shape[1]\n",
    "    model = GQNN(in_channels=in_channels, hidden_channels=hidden_channels).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "    criterion = QRLoss()\n",
    "    train_data = train_data.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if q == \"all\":\n",
    "            taus = torch.rand(train_data.x.size(0), 1, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            taus = torch.full((train_data.x.size(0), 1), q, dtype=torch.float32, device=device)\n",
    "\n",
    "        preds = model(train_data.x, train_data.edge_index, taus)\n",
    "\n",
    "        loss = criterion(preds, train_data.y, taus)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % (num_epochs // 10) == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_dynamic_gqnn(train_data, q='all', tau_low=0.05, tau_upper=0.95, hidden_channels=64, num_epochs=1000, learning_rate=0.001, weight=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    in_channels = train_data.x.shape[1]\n",
    "    model = GQNN(in_channels=in_channels, hidden_channels=hidden_channels).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "    criterion = QRLoss()\n",
    "    train_data = train_data.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if q == \"all\":\n",
    "            taus_low = (torch.rand(train_data.x.size(0), 1) * (0.5 - tau_low) + tau_low).to(device)\n",
    "            taus_upper = (torch.rand(train_data.x.size(0), 1) * (tau_upper - 0.5) + 0.5).to(device)\n",
    "        elif q == 'single':\n",
    "            taus_low = torch.full((train_data.x.size(0), 1), tau_low, device=device)\n",
    "            taus_upper = torch.full((train_data.x.size(0), 1), tau_upper, device=device)\n",
    "\n",
    "        preds_low = model(train_data.x, train_data.edge_index, taus_low)\n",
    "        preds_upper = model(train_data.x, train_data.edge_index, taus_upper)\n",
    "        \n",
    "        loss_low = criterion(preds_low, train_data.y, taus_low)\n",
    "        loss_upper = criterion(preds_upper, train_data.y, taus_upper)\n",
    "        loss = loss_low + loss_upper\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        coverage = ((train_data.y >= preds_low) & (train_data.y <= preds_upper)).float().mean().item()\n",
    "        target_coverage = 0.9\n",
    "        tau_low, tau_upper = dynamic_quantile_adjustment(tau_low, tau_upper, coverage, target=target_coverage)\n",
    "        \n",
    "        if epoch % (num_epochs // 10) == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, Coverage Error: {target_coverage - coverage:.4f}\")\n",
    "\n",
    "    return model, tau_low, tau_upper\n",
    "\n",
    "def train_pi_gqnn(train_data, hidden_channels=64, num_epochs=1000, learning_rate=0.001, weight=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    in_channels = train_data.x.shape[1]\n",
    "    model = GQNN_PI(in_channels=in_channels, hidden_channels=hidden_channels).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "    criterion = RQRLoss()\n",
    "    train_data = train_data.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(train_data.x, train_data.edge_index)\n",
    "        loss = criterion(preds[:, 0], preds[:, 1], train_data.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % (num_epochs // 10) == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 800 nodes, 1279 edges\n",
      "Test data: 200 nodes, 84 edges\n",
      "Epoch 0: Loss = 0.3740\n",
      "Epoch 100: Loss = 0.0848\n",
      "Epoch 200: Loss = 0.0786\n"
     ]
    }
   ],
   "source": [
    "train_data_bs, test_data_bs = split_graph_data(graph_data_basic)\n",
    "\n",
    "model_fixed_all = train_fixed_gqnn(train_data_bs, q='all')\n",
    "model_fixed_05 = train_fixed_gqnn(train_data_bs, q=0.05)\n",
    "model_fixed_95 = train_fixed_gqnn(train_data_bs, q=0.95)\n",
    "\n",
    "model_dynamic_all = train_dynamic_gqnn(train_data_bs, q='all')\n",
    "model_dynamic_single = train_dynamic_gqnn(train_data_bs, q='single')\n",
    "\n",
    "model_pi = train_pi_gqnn(train_data_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 800 nodes, 1259 edges\n",
      "Test data: 200 nodes, 78 edges\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "import torch\n",
    "\n",
    "# 데이터 분할 (RandomNodeSplit 사용)\n",
    "transform = RandomNodeSplit(split=\"train_rest\", num_val=0.0, num_test=0.2)\n",
    "data = transform(graph_data_basic)\n",
    "\n",
    "# Train/Test 마스크 생성\n",
    "train_mask = data.train_mask.to(data.x.device)\n",
    "test_mask = data.test_mask.to(data.x.device)\n",
    "\n",
    "# 훈련 및 테스트 데이터 선택\n",
    "train_data = data.clone()\n",
    "test_data = data.clone()\n",
    "\n",
    "train_data.x = data.x[train_mask]\n",
    "train_data.y = data.y[train_mask]\n",
    "test_data.x = data.x[test_mask]\n",
    "test_data.y = data.y[test_mask]\n",
    "\n",
    "# 노드 인덱스를 매핑하여 edge_index 수정 (훈련 데이터)\n",
    "train_node_mapping = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(train_mask.nonzero(as_tuple=True)[0])}\n",
    "train_edges_mask = train_mask[data.edge_index[0]] & train_mask[data.edge_index[1]]\n",
    "train_edges = data.edge_index[:, train_edges_mask]\n",
    "\n",
    "train_data.edge_index = torch.stack([\n",
    "    torch.tensor([train_node_mapping[i.item()] for i in train_edges[0]], dtype=torch.long, device=data.x.device),\n",
    "    torch.tensor([train_node_mapping[i.item()] for i in train_edges[1]], dtype=torch.long, device=data.x.device)\n",
    "], dim=0)\n",
    "\n",
    "# 노드 인덱스를 매핑하여 edge_index 수정 (테스트 데이터)\n",
    "test_node_mapping = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(test_mask.nonzero(as_tuple=True)[0])}\n",
    "test_edges_mask = test_mask[data.edge_index[0]] & test_mask[data.edge_index[1]]\n",
    "test_edges = data.edge_index[:, test_edges_mask]\n",
    "\n",
    "test_data.edge_index = torch.stack([\n",
    "    torch.tensor([test_node_mapping[i.item()] for i in test_edges[0]], dtype=torch.long, device=data.x.device),\n",
    "    torch.tensor([test_node_mapping[i.item()] for i in test_edges[1]], dtype=torch.long, device=data.x.device)\n",
    "], dim=0)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Train data: {train_data.x.shape[0]} nodes, {train_data.edge_index.shape[1]} edges\")\n",
    "print(f\"Test data: {test_data.x.shape[0]} nodes, {test_data.edge_index.shape[1]} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 5])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주변 노드 정보를 고려하여 신뢰 구간을 학습하는 GNN\n",
    "# 고정된 분위수가 아닌 모델이 학습하면서 신뢰구간을 예측하도록 설계 -> 유연한 방식\n",
    "class QuantileGNNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(QuantileGNNLayer, self).__init__()\n",
    "        self.conv = SAGEConv(in_channels, hidden_channels)  # 그래프 메세지 패싱을 수행하는 레이어\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)  # 최종 출력을 위한 선형 변환\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv(x, edge_index)  # GNN 메세지 패싱 수행\n",
    "        h = F.relu(h)   # 활성화 함수 적용 (비선형 변환)\n",
    "        h = self.fc(h)  # 최종 분위수 얘측\n",
    "        return h  # (num_nodes, 2) 반환 (ql qu)\n",
    "\n",
    "# 신뢰 구간을 예측하는 GNN 모델\n",
    "class QuantileGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=2):\n",
    "        super(QuantileGNN, self).__init__()\n",
    "        self.layer1 = QuantileGNNLayer(input_dim, hidden_dim, hidden_dim)\n",
    "        self.layer2 = QuantileGNNLayer(hidden_dim, hidden_dim, output_dim)  # 최종 출력 2차원 (q1, q2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.layer1(x, edge_index))\n",
    "        x = self.layer2(x, edge_index)  # 최종 분위수 출력\n",
    "        return x  # (num_nodes, 2) 반환\n",
    "\n",
    "class DynamicRQRLoss(nn.Module):\n",
    "    def __init__(self, target_coverage=0.9, lambda_factor=0.1):\n",
    "        super(DynamicRQRLoss, self).__init__()\n",
    "        self.target_coverage = target_coverage\n",
    "        self.lambda_factor = lambda_factor\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        q1, q2 = preds[:, 0], preds[:, 1]\n",
    "        width = q2 - q1\n",
    "\n",
    "        # 기존 RQRW Loss (완화된 분위수 회귀)\n",
    "        diff_mu_1 = target - q1\n",
    "        diff_mu_2 = target - q2\n",
    "        rqr_loss = torch.maximum(diff_mu_1 * diff_mu_2 * (self.target_coverage + 2 * self.lambda_factor),\n",
    "                                 diff_mu_2 * diff_mu_1 * (self.target_coverage + 2 * self.lambda_factor - 1))\n",
    "\n",
    "        # 신뢰구간 너비 최소화 정규화\n",
    "        width_penalty = self.lambda_factor * torch.square(width) * 0.5\n",
    "        \n",
    "        # 분위수 교차 방지?\n",
    "\n",
    "        return torch.mean(rqr_loss + width_penalty)\n",
    "\n",
    "def train_quantile_gnn(graph_data, train_data, epochs=1000, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 모델 초기화\n",
    "    input_dim = graph_data.x.shape[1]\n",
    "    model = QuantileGNN(input_dim=input_dim, hidden_dim=64).to(device)\n",
    "    criterion = DynamicRQRLoss(target_coverage=0.9, lambda_factor=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_mask = torch.tensor([i in train_data for i in range(graph_data.x.shape[0])])\n",
    "    \n",
    "    graph_data = graph_data.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(graph_data)\n",
    "        loss = criterion(preds[train_mask], graph_data.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step() # 모델 업데이트\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_quantile_gnn(model, graph_data, test_data):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "\n",
    "    test_mask = torch.tensor([i in test_data for i in range(graph_data.x.shape[0])])\n",
    "    graph_data = graph_data.to(device)\n",
    "\n",
    "    with torch.no_grad():  # 그래디언트 계산 없이 예측 수행\n",
    "        test_preds = model(graph_data)[test_mask]  # (num_test_nodes, 2) → 분위수 예측값\n",
    "        test_targets = graph_data.y[test_mask]  # 실제 정답\n",
    "\n",
    "    return test_preds.cpu().numpy(), test_targets.cpu().numpy()\n",
    "\n",
    "def evaluate_model_performance(test_preds, test_targets, coverage_target=0.9):\n",
    "    ql = test_preds[:, 0]  # 신뢰구간 하한 (예측값)\n",
    "    qu = test_preds[:, 1]  # 신뢰구간 상한 (예측값)\n",
    "    \n",
    "    coverage = np.mean((test_targets >= ql) & (test_targets <= qu))\n",
    "\n",
    "    interval_width = np.mean(qu - ql)\n",
    "    \n",
    "    mae = np.mean(np.abs((ql + qu) / 2 - test_targets))  # 신뢰구간 중앙값과 실제 값 비교\n",
    "\n",
    "    print(f\"   - Coverage Rate: {coverage:.4f} (목표: {coverage_target})\")\n",
    "    print(f\"   - Prediction Interval Width: {interval_width:.4f}\")\n",
    "    print(f\"   - Mean Absolute Error (MAE): {mae:.4f}\\n\")\n",
    "\n",
    "    # return coverage, interval_width, mae\n",
    "\n",
    "def advanced_evaluate_model(test_preds, test_targets, coverage_target=0.9):\n",
    "    \"\"\"\n",
    "    추가적인 평가 지표를 포함한 모델 평가 함수\n",
    "    \"\"\"\n",
    "    q1 = test_preds[:, 0]  # 신뢰구간 하한\n",
    "    q2 = test_preds[:, 1]  # 신뢰구간 상한\n",
    "    median_pred = (q1 + q2) / 2  # 신뢰구간 중앙값\n",
    "\n",
    "    coverage = np.mean((test_targets >= q1) & (test_targets <= q2))\n",
    "\n",
    "    interval_width = np.mean(q2 - q1)\n",
    "\n",
    "    mae = np.mean(np.abs(median_pred - test_targets))\n",
    "\n",
    "    sharpness = np.mean(np.square(q2 - q1))\n",
    "\n",
    "    alpha = 1 - coverage_target  # 신뢰구간 목표\n",
    "    lower_penalty = np.maximum(q1 - test_targets, 0) * (alpha / 2)\n",
    "    upper_penalty = np.maximum(test_targets - q2, 0) * (alpha / 2)\n",
    "    interval_score = interval_width + 2 * (lower_penalty + upper_penalty).mean()\n",
    "\n",
    "    def pinball_loss(y_true, y_pred, quantile):\n",
    "        return np.maximum(quantile * (y_true - y_pred), (quantile - 1) * (y_true - y_pred)).mean()\n",
    "    \n",
    "    pinball_loss_lower = pinball_loss(test_targets, q1, alpha / 2)\n",
    "    pinball_loss_upper = pinball_loss(test_targets, q2, 1 - alpha / 2)\n",
    "\n",
    "    calibration_error = np.abs(coverage - coverage_target)\n",
    "\n",
    "    print(f\"   - Coverage Rate: {coverage:.4f} (목표: {coverage_target})\")\n",
    "    print(f\"   - Prediction Interval Width (PIW): {interval_width:.4f}\")\n",
    "    print(f\"   - Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"   - Sharpness: {sharpness:.4f} (작을수록 좋음)\")\n",
    "    print(f\"   - Interval Score (IS): {interval_score:.4f} (작을수록 좋음)\")\n",
    "    print(f\"   - Pinball Loss (Lower): {pinball_loss_lower:.4f}, (Upper): {pinball_loss_upper:.4f}\")\n",
    "    print(f\"   - Calibration Error: {calibration_error:.4f} (0에 가까울수록 좋음)\\n\")\n",
    "\n",
    "    return {\n",
    "        \"coverage\": coverage,\n",
    "        \"interval_width\": interval_width,\n",
    "        \"mae\": mae,\n",
    "        \"sharpness\": sharpness,\n",
    "        \"interval_score\": interval_score,\n",
    "        \"pinball_loss_lower\": pinball_loss_lower,\n",
    "        \"pinball_loss_upper\": pinball_loss_upper,\n",
    "        \"calibration_error\": calibration_error\n",
    "    }\n",
    "\n",
    "def visualize_results(preds, targets, title=\"Quantile GNN - Prediction Intervals\"):\n",
    "    lower_bound = preds[:, 0]  # 신뢰구간 하한\n",
    "    upper_bound = preds[:, 1]  # 신뢰구간 상한\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(targets, label=\"True Values\", marker='o', alpha = 0.5)\n",
    "    plt.fill_between(range(len(targets)), lower_bound, upper_bound, color='blue', alpha=0.3, label=\"Prediction Intervals(target coverage = 90%)\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Node Index\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def experiment_results_to_df(experiment_results):\n",
    "    data = []\n",
    "    for noise_type, results in experiment_results.items():\n",
    "        train_metrics = results[\"train\"]\n",
    "        test_metrics = results[\"test\"]\n",
    "        \n",
    "        data.append([\n",
    "            noise_type.upper(),  \n",
    "            train_metrics[\"coverage\"], test_metrics[\"coverage\"],  \n",
    "            train_metrics[\"interval_width\"], test_metrics[\"interval_width\"],\n",
    "            train_metrics[\"mae\"], test_metrics[\"mae\"],\n",
    "            train_metrics[\"interval_score\"], test_metrics[\"interval_score\"],\n",
    "        ])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=[\n",
    "        \"Noise Type\",  \n",
    "        \"Train Coverage\", \"Test Coverage\",  \n",
    "        \"Train Interval Width\", \"Test Interval Width\",\n",
    "        \"Train MAE\", \"Test MAE\",\n",
    "        \"Train Interval Score\", \"Test Interval Score\"\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_experiment_results(experiment_df):\n",
    "    noise_types = experiment_df[\"Noise Type\"]\n",
    "    \n",
    "    metrics = [\n",
    "        (\"Train Coverage\", \"Test Coverage\"),\n",
    "        (\"Train Interval Width\", \"Test Interval Width\"),\n",
    "        (\"Train MAE\", \"Test MAE\"),\n",
    "        (\"Train Interval Score\", \"Test Interval Score\")\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (train_metric, test_metric) in enumerate(metrics):\n",
    "        train_values = experiment_df[train_metric]\n",
    "        test_values = experiment_df[test_metric]\n",
    "        \n",
    "        x = np.arange(len(noise_types))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[i].bar(x - width/2, train_values, width, label=\"Train\")\n",
    "        axes[i].bar(x + width/2, test_values, width, label=\"Test\")\n",
    "        \n",
    "        axes[i].set_xticks(x)\n",
    "        axes[i].set_xticklabels(noise_types, rotation=45)\n",
    "        axes[i].set_title(f\"{train_metric} vs {test_metric}\")\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "train_data_bs, test_data_bs = train_test_split(range(graph_data_bs.x.shape[0]), test_size=0.2, random_state=127)\n",
    "model_bs = train_quantile_gnn(graph_data_bs, train_data_bs, epochs=1000, learning_rate=0.001)\n",
    "\n",
    "# 테스트\n",
    "train_preds_bs, train_targets_bs = test_quantile_gnn(model_bs, graph_data_bs, train_data_bs)\n",
    "test_preds_bs, test_targets_bs = test_quantile_gnn(model_bs, graph_data_bs, test_data_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train evaluation')\n",
    "advanced_evaluate_model(train_preds_bs, train_targets_bs)\n",
    "print('-' * 50)\n",
    "print('test evaluation')\n",
    "advanced_evaluate_model(test_preds_bs, test_targets_bs)\n",
    "print('-' * 50)\n",
    "print('Visualization')\n",
    "visualize_results(train_preds_bs, train_targets_bs, title=\"Train Data - Prediction Intervals\")\n",
    "visualize_results(test_preds_bs, test_targets_bs, title=\"Test Data - Prediction Intervals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gqnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
